{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7051d64e-d389-4f2c-b8c1-6c601160d4c8",
   "metadata": {},
   "source": [
    "# Tree ensembles\n",
    "\n",
    "## Contents in this notebook:\n",
    "1. [Notebook setup](#setup)\n",
    "2. [Decision tree](#tree)\n",
    "3. [Random forest](#forest)\n",
    "5. [Gradient boosting](#xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c56f61-a853-4177-ab41-a1e0d8a05c7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Notebook setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f19c1bb-0459-4fcb-b95e-164bf4ce95b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This folder: C:\\Users\\muell\\thesis_main\\thesis_code\\notebooks\n",
      "Project folder: C:\\Users\\muell\\thesis_main\\thesis_code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "# !pip install requirements.txt\n",
    "\n",
    "# Change directory to main project folder.\n",
    "import os\n",
    "print(f'This folder: {os.getcwd()}')\n",
    "os.chdir(\"..\")\n",
    "print(f'Project folder: {os.getcwd()}\\n')\n",
    "\n",
    "# Import packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dill as pickle\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold, ParameterGrid, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea810c23-96cd-41cf-a9ee-ffca5aaef25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 43) (80000,)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "\n",
    "X_train = pd.read_pickle('data/generated_final_pickle/df_features_train.pickle')\n",
    "y_train = X_train.pop('distance')\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac5404-4e75-405f-98c9-2ade66f30486",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea24f5-255e-474e-b243-6e658b25f236",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Decision tree <a name=\"tree\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ad61a9-20d1-40c8-a777-35fd08aa58d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (mean, std): 1671.421 (18.811)\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeRegressor\n",
    "regr = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# cross validation\n",
    "scores_tree = abs(cross_val_score(\n",
    "    regr, X_train, y_train, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    cv=KFold(n_splits=5), verbose=0, n_jobs=5))\n",
    "print(f'RMSE (mean, std): %.3f (%.3f)' % (scores_tree.mean(), scores_tree.std()))\n",
    "\n",
    "# store result\n",
    "results['DecisionTreeRegressor'] = [scores_tree.mean(), scores_tree.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937399d0-e5ad-4a44-806d-3a654a397644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean(RMSE)</th>\n",
       "      <th>std(RMSE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>1671.421257</td>\n",
       "      <td>18.811115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean(RMSE)  std(RMSE)\n",
       "DecisionTreeRegressor  1671.421257  18.811115"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=['mean(RMSE)', 'std(RMSE)']).T.sort_values('mean(RMSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d03a500-0d29-4700-84bf-24606957bce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x273ec200490>,\n",
       "  <matplotlib.lines.Line2D at 0x273ec200760>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x273ec200af0>,\n",
       "  <matplotlib.lines.Line2D at 0x273ec200d00>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x273ec2001c0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x273ec231a90>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x273ec231df0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(scores_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51501d6b-0dd3-4ac4-aef6-e84647e6faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "regr.fit(X_train, y_train)\n",
    "# with open('models/decision_tree.pkl', 'wb') as f:\n",
    "#     pickle.dump(regr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c26328-4ca0-4882-bbea-f4702e643ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.875, 'NumVehMin <= 15.857\\nsquared_error = 30571015.869\\nsamples = 80000\\nvalue = 7736.603'),\n",
       " Text(0.25, 0.625, 'Perimeter <= 2234.732\\nsquared_error = 16020352.331\\nsamples = 76443\\nvalue = 7105.11'),\n",
       " Text(0.125, 0.375, 'Perimeter <= 1425.832\\nsquared_error = 6086601.976\\nsamples = 38357\\nvalue = 4722.766'),\n",
       " Text(0.0625, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.1875, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.375, 0.375, 'NumCust <= 58.5\\nsquared_error = 14552225.352\\nsamples = 38086\\nvalue = 9504.405'),\n",
       " Text(0.3125, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.4375, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.75, 0.625, 'Perimeter <= 2185.995\\nsquared_error = 150526206.948\\nsamples = 3557\\nvalue = 21307.93'),\n",
       " Text(0.625, 0.375, 'Perimeter <= 1355.606\\nsquared_error = 68346635.993\\nsamples = 1660\\nvalue = 14456.957'),\n",
       " Text(0.5625, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.6875, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.875, 0.375, 'NumVehMin <= 29.031\\nsquared_error = 145426140.043\\nsamples = 1897\\nvalue = 27302.983'),\n",
       " Text(0.8125, 0.125, '\\n  (...)  \\n'),\n",
       " Text(0.9375, 0.125, '\\n  (...)  \\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize\n",
    "\n",
    "print(regr.tree_.max_depth)\n",
    "\n",
    "plt.figure(figsize=(25,15))\n",
    "plot_tree(regr, filled=True, max_depth=2, rounded=True, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bdaa5-4e71-45ff-a38c-7ca55f4a1e5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Random forest <a name=\"forest\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9d1ffc-bccd-4f33-a352-6c07f31f6f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (mean, std): 1008.687 (33.793)\n"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "regr = RandomForestRegressor(verbose=3, random_state=42)\n",
    "\n",
    "# cross validation\n",
    "scores = abs(cross_val_score(\n",
    "    regr, X_train, y_train, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    cv=KFold(n_splits=5), verbose=0, n_jobs=5))\n",
    "print(f'RMSE (mean, std): %.3f (%.3f)' % (scores.mean(), scores.std()))\n",
    "\n",
    "# store result\n",
    "results['RandomForestRegressor'] = [scores.mean(), scores.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4cbcaa-e7d1-473e-8e0f-313b5d5220e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean(RMSE)</th>\n",
       "      <th>std(RMSE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>1008.686744</td>\n",
       "      <td>33.792708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>1671.421257</td>\n",
       "      <td>18.811115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean(RMSE)  std(RMSE)\n",
       "RandomForestRegressor  1008.686744  33.792708\n",
       "DecisionTreeRegressor  1671.421257  18.811115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=['mean(RMSE)', 'std(RMSE)']).T.sort_values('mean(RMSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bfd0557-c22c-4083-995f-90a577d10660",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:   41.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:  2.8min finished\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "regr = RandomForestRegressor(verbose=3, random_state=42, n_jobs=5)\n",
    "regr.fit(X_train, y_train)\n",
    "with open('models/ensemble_random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(regr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a602c5-5b89-4743-9b84-6327d9301683",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Determine n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2c2fbae-761a-4168-9741-a865e10cf085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 10, RMSE (mean, std): 1095.891 (44.085)\n",
      "n_estimators: 20, RMSE (mean, std): 1053.925 (43.743)\n",
      "n_estimators: 50, RMSE (mean, std): 1020.606 (38.527)\n",
      "n_estimators: 100, RMSE (mean, std): 1008.687 (33.793)\n"
     ]
    }
   ],
   "source": [
    "for i in [10, 20, 50, 100]:    # XGBRegressor\n",
    "    \n",
    "    regr = RandomForestRegressor(verbose=3, random_state=42, n_estimators = i, n_jobs=5)\n",
    "\n",
    "    # cross validation\n",
    "    scores_rf = abs(cross_val_score(\n",
    "        regr, X_train, y_train, \n",
    "        scoring='neg_root_mean_squared_error', \n",
    "        cv=KFold(n_splits=5), verbose=0, n_jobs=5))\n",
    "    print(f'n_estimators: {i}, RMSE (mean, std): %.3f (%.3f)' % (scores_rf.mean(), scores_rf.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8dead5-ce81-40e2-97a4-a8c907f4d83e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983d98cd-e2f2-4a5e-abd2-924d2565dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_features': ['auto', 'sqrt'], \n",
    "    'max_depth': [None, 10, 20, 30, 40, 50], \n",
    "    'min_samples_split': range(2, 10), \n",
    "    'min_samples_leaf': range(2, 6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfaceef-53f4-4be1-801b-a0e43df7c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=20)\n",
    "\n",
    "param_comb=30\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=2, \n",
    "    n_jobs=4, \n",
    "    cv=4, \n",
    "    random_state=42, \n",
    "    n_iter=param_comb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b96937-5416-4a53-b815-64bf571f598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 30 candidates, totalling 120 fits\n",
      "Time elapsed: 1264.033763408661\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Time elapsed:', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f661337b-b4ec-4b14-b953-4ac460327c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score\n",
      "-1051.0551595449676\n",
      "\n",
      " Best hyperparameters:\n",
      "{'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "print('\\n Best score')\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('models/rf-random-grid-search-results-cycle1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58717a5-55d4-4ff1-a9fd-b42132d28709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>91.929058</td>\n",
       "      <td>0.120003</td>\n",
       "      <td>0.207871</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 2, 'min_samples_leaf': 2...</td>\n",
       "      <td>-1001.236490</td>\n",
       "      <td>-1058.934057</td>\n",
       "      <td>-1058.854094</td>\n",
       "      <td>-1085.195997</td>\n",
       "      <td>-1051.055160</td>\n",
       "      <td>30.701779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>88.871020</td>\n",
       "      <td>0.638347</td>\n",
       "      <td>0.188541</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{'min_samples_split': 7, 'min_samples_leaf': 2...</td>\n",
       "      <td>-1002.447422</td>\n",
       "      <td>-1063.889973</td>\n",
       "      <td>-1062.411694</td>\n",
       "      <td>-1076.725722</td>\n",
       "      <td>-1051.368703</td>\n",
       "      <td>28.788018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.701334</td>\n",
       "      <td>0.337682</td>\n",
       "      <td>0.194501</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{'min_samples_split': 6, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1009.901878</td>\n",
       "      <td>-1073.509704</td>\n",
       "      <td>-1049.872131</td>\n",
       "      <td>-1077.139125</td>\n",
       "      <td>-1052.605709</td>\n",
       "      <td>26.785981</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>90.392931</td>\n",
       "      <td>0.203643</td>\n",
       "      <td>0.199001</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>30.0</td>\n",
       "      <td>{'min_samples_split': 5, 'min_samples_leaf': 2...</td>\n",
       "      <td>-991.921014</td>\n",
       "      <td>-1065.153861</td>\n",
       "      <td>-1069.536650</td>\n",
       "      <td>-1083.942226</td>\n",
       "      <td>-1052.638438</td>\n",
       "      <td>35.737648</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>86.889419</td>\n",
       "      <td>0.309407</td>\n",
       "      <td>0.181209</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1020.296021</td>\n",
       "      <td>-1083.006077</td>\n",
       "      <td>-1053.042873</td>\n",
       "      <td>-1090.846133</td>\n",
       "      <td>-1061.797776</td>\n",
       "      <td>27.805769</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.676844</td>\n",
       "      <td>0.400644</td>\n",
       "      <td>0.195603</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 6, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1004.743013</td>\n",
       "      <td>-1070.711113</td>\n",
       "      <td>-1078.344092</td>\n",
       "      <td>-1094.151952</td>\n",
       "      <td>-1061.987542</td>\n",
       "      <td>34.114230</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>89.914011</td>\n",
       "      <td>0.207458</td>\n",
       "      <td>0.194461</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 3, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1023.468863</td>\n",
       "      <td>-1067.751193</td>\n",
       "      <td>-1074.516531</td>\n",
       "      <td>-1084.435310</td>\n",
       "      <td>-1062.542974</td>\n",
       "      <td>23.326766</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>85.094247</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.166660</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>20.0</td>\n",
       "      <td>{'min_samples_split': 2, 'min_samples_leaf': 5...</td>\n",
       "      <td>-1020.639787</td>\n",
       "      <td>-1084.559108</td>\n",
       "      <td>-1072.668320</td>\n",
       "      <td>-1085.179196</td>\n",
       "      <td>-1065.761602</td>\n",
       "      <td>26.523905</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>87.623551</td>\n",
       "      <td>0.448656</td>\n",
       "      <td>0.177240</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>20.0</td>\n",
       "      <td>{'min_samples_split': 3, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1010.414084</td>\n",
       "      <td>-1085.083943</td>\n",
       "      <td>-1067.449167</td>\n",
       "      <td>-1101.869405</td>\n",
       "      <td>-1066.204150</td>\n",
       "      <td>34.433046</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14.491652</td>\n",
       "      <td>0.075431</td>\n",
       "      <td>0.201788</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 6, 'min_samples_leaf': 2...</td>\n",
       "      <td>-1240.155530</td>\n",
       "      <td>-1291.924793</td>\n",
       "      <td>-1295.844133</td>\n",
       "      <td>-1313.993573</td>\n",
       "      <td>-1285.479507</td>\n",
       "      <td>27.460344</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.196118</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.195745</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 2, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1233.635181</td>\n",
       "      <td>-1289.931796</td>\n",
       "      <td>-1318.570861</td>\n",
       "      <td>-1312.656963</td>\n",
       "      <td>-1288.698701</td>\n",
       "      <td>33.540483</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.204522</td>\n",
       "      <td>0.206278</td>\n",
       "      <td>0.215025</td>\n",
       "      <td>0.006678</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 5, 'min_samples_leaf': 2...</td>\n",
       "      <td>-1273.613252</td>\n",
       "      <td>-1257.522394</td>\n",
       "      <td>-1328.496327</td>\n",
       "      <td>-1299.736395</td>\n",
       "      <td>-1289.842092</td>\n",
       "      <td>26.925755</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.443248</td>\n",
       "      <td>0.106052</td>\n",
       "      <td>0.183197</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1246.858989</td>\n",
       "      <td>-1288.766951</td>\n",
       "      <td>-1330.180755</td>\n",
       "      <td>-1322.168002</td>\n",
       "      <td>-1296.993674</td>\n",
       "      <td>32.849257</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.137743</td>\n",
       "      <td>0.063913</td>\n",
       "      <td>0.184962</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 2...</td>\n",
       "      <td>-1255.531304</td>\n",
       "      <td>-1308.201702</td>\n",
       "      <td>-1316.357329</td>\n",
       "      <td>-1310.796740</td>\n",
       "      <td>-1297.721769</td>\n",
       "      <td>24.536215</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.929653</td>\n",
       "      <td>0.107092</td>\n",
       "      <td>0.180781</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1232.206354</td>\n",
       "      <td>-1323.099634</td>\n",
       "      <td>-1327.773148</td>\n",
       "      <td>-1309.233393</td>\n",
       "      <td>-1298.078132</td>\n",
       "      <td>38.637419</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64.169162</td>\n",
       "      <td>0.271193</td>\n",
       "      <td>0.092249</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{'min_samples_split': 7, 'min_samples_leaf': 5...</td>\n",
       "      <td>-1263.202128</td>\n",
       "      <td>-1303.637086</td>\n",
       "      <td>-1296.203022</td>\n",
       "      <td>-1330.821215</td>\n",
       "      <td>-1298.465863</td>\n",
       "      <td>24.094926</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.953693</td>\n",
       "      <td>0.323392</td>\n",
       "      <td>0.188685</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{'min_samples_split': 6, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1231.557140</td>\n",
       "      <td>-1294.157054</td>\n",
       "      <td>-1344.850336</td>\n",
       "      <td>-1327.280501</td>\n",
       "      <td>-1299.461258</td>\n",
       "      <td>43.223785</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.541475</td>\n",
       "      <td>0.109409</td>\n",
       "      <td>0.182127</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30.0</td>\n",
       "      <td>{'min_samples_split': 7, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1231.467465</td>\n",
       "      <td>-1310.421398</td>\n",
       "      <td>-1353.717812</td>\n",
       "      <td>-1306.039563</td>\n",
       "      <td>-1300.411559</td>\n",
       "      <td>43.950870</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.917714</td>\n",
       "      <td>0.127773</td>\n",
       "      <td>0.215215</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 3, 'min_samples_leaf': 2...</td>\n",
       "      <td>-1225.319166</td>\n",
       "      <td>-1305.919640</td>\n",
       "      <td>-1346.034638</td>\n",
       "      <td>-1326.132472</td>\n",
       "      <td>-1300.851479</td>\n",
       "      <td>45.857014</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.440363</td>\n",
       "      <td>0.050691</td>\n",
       "      <td>0.195492</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 4, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1226.940001</td>\n",
       "      <td>-1327.176307</td>\n",
       "      <td>-1297.349799</td>\n",
       "      <td>-1361.604226</td>\n",
       "      <td>-1303.267583</td>\n",
       "      <td>49.587577</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.833883</td>\n",
       "      <td>0.056881</td>\n",
       "      <td>0.187515</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30.0</td>\n",
       "      <td>{'min_samples_split': 8, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1238.590213</td>\n",
       "      <td>-1333.160455</td>\n",
       "      <td>-1303.246706</td>\n",
       "      <td>-1345.450902</td>\n",
       "      <td>-1305.112069</td>\n",
       "      <td>41.359915</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.957353</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>0.185653</td>\n",
       "      <td>0.008846</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50.0</td>\n",
       "      <td>{'min_samples_split': 6, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1223.932771</td>\n",
       "      <td>-1327.438985</td>\n",
       "      <td>-1323.655617</td>\n",
       "      <td>-1349.931627</td>\n",
       "      <td>-1306.239750</td>\n",
       "      <td>48.569895</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.837701</td>\n",
       "      <td>0.110709</td>\n",
       "      <td>0.184009</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1239.716483</td>\n",
       "      <td>-1323.031124</td>\n",
       "      <td>-1338.907060</td>\n",
       "      <td>-1340.866300</td>\n",
       "      <td>-1310.630242</td>\n",
       "      <td>41.522105</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.530377</td>\n",
       "      <td>0.088262</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20.0</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1219.649308</td>\n",
       "      <td>-1368.606596</td>\n",
       "      <td>-1339.048832</td>\n",
       "      <td>-1326.119557</td>\n",
       "      <td>-1313.356073</td>\n",
       "      <td>56.250783</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.581076</td>\n",
       "      <td>0.131116</td>\n",
       "      <td>0.189683</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30.0</td>\n",
       "      <td>{'min_samples_split': 4, 'min_samples_leaf': 5...</td>\n",
       "      <td>-1243.198180</td>\n",
       "      <td>-1344.860458</td>\n",
       "      <td>-1367.554583</td>\n",
       "      <td>-1334.200795</td>\n",
       "      <td>-1322.453504</td>\n",
       "      <td>47.316974</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.717330</td>\n",
       "      <td>0.099467</td>\n",
       "      <td>0.182085</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30.0</td>\n",
       "      <td>{'min_samples_split': 9, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1292.142023</td>\n",
       "      <td>-1349.195267</td>\n",
       "      <td>-1326.605038</td>\n",
       "      <td>-1343.903711</td>\n",
       "      <td>-1327.961510</td>\n",
       "      <td>22.304158</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.645595</td>\n",
       "      <td>0.094296</td>\n",
       "      <td>0.193327</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'min_samples_split': 2, 'min_samples_leaf': 5...</td>\n",
       "      <td>-1219.412034</td>\n",
       "      <td>-1421.238612</td>\n",
       "      <td>-1338.840104</td>\n",
       "      <td>-1360.183488</td>\n",
       "      <td>-1334.918559</td>\n",
       "      <td>73.223296</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.530014</td>\n",
       "      <td>0.065832</td>\n",
       "      <td>0.094371</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{'min_samples_split': 8, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1588.587843</td>\n",
       "      <td>-1664.176200</td>\n",
       "      <td>-1682.263488</td>\n",
       "      <td>-1632.921829</td>\n",
       "      <td>-1641.987340</td>\n",
       "      <td>35.525355</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.676431</td>\n",
       "      <td>0.081732</td>\n",
       "      <td>0.097548</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{'min_samples_split': 4, 'min_samples_leaf': 4...</td>\n",
       "      <td>-1579.663306</td>\n",
       "      <td>-1660.021871</td>\n",
       "      <td>-1662.073398</td>\n",
       "      <td>-1679.004942</td>\n",
       "      <td>-1645.190879</td>\n",
       "      <td>38.542935</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.570001</td>\n",
       "      <td>0.091679</td>\n",
       "      <td>0.094518</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>10.0</td>\n",
       "      <td>{'min_samples_split': 8, 'min_samples_leaf': 3...</td>\n",
       "      <td>-1613.077897</td>\n",
       "      <td>-1635.479698</td>\n",
       "      <td>-1719.187057</td>\n",
       "      <td>-1634.533892</td>\n",
       "      <td>-1650.569636</td>\n",
       "      <td>40.616598</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "28      91.929058      0.120003         0.207871        0.003878   \n",
       "13      88.871020      0.638347         0.188541        0.002441   \n",
       "0       88.701334      0.337682         0.194501        0.013049   \n",
       "12      90.392931      0.203643         0.199001        0.003534   \n",
       "24      86.889419      0.309407         0.181209        0.004713   \n",
       "3       91.676844      0.400644         0.195603        0.005861   \n",
       "22      89.914011      0.207458         0.194461        0.005259   \n",
       "16      85.094247      0.401600         0.166660        0.002340   \n",
       "26      87.623551      0.448656         0.177240        0.004643   \n",
       "27      14.491652      0.075431         0.201788        0.005259   \n",
       "17      14.196118      0.125004         0.195745        0.004360   \n",
       "2       16.204522      0.206278         0.215025        0.006678   \n",
       "8       13.443248      0.106052         0.183197        0.001471   \n",
       "21      14.137743      0.063913         0.184962        0.003195   \n",
       "18      13.929653      0.107092         0.180781        0.002114   \n",
       "23      64.169162      0.271193         0.092249        0.003629   \n",
       "29      13.953693      0.323392         0.188685        0.005891   \n",
       "10      13.541475      0.109409         0.182127        0.001820   \n",
       "6       14.917714      0.127773         0.215215        0.002032   \n",
       "20      14.440363      0.050691         0.195492        0.003507   \n",
       "9       13.833883      0.056881         0.187515        0.004801   \n",
       "5       13.957353      0.068413         0.185653        0.008846   \n",
       "25      13.837701      0.110709         0.184009        0.000692   \n",
       "15      13.530377      0.088262         0.184240        0.002599   \n",
       "1       14.581076      0.131116         0.189683        0.011596   \n",
       "19      13.717330      0.099467         0.182085        0.007383   \n",
       "4       13.645595      0.094296         0.193327        0.011330   \n",
       "11       9.530014      0.065832         0.094371        0.005939   \n",
       "7        9.676431      0.081732         0.097548        0.001627   \n",
       "14       9.570001      0.091679         0.094518        0.006019   \n",
       "\n",
       "    param_min_samples_split  param_min_samples_leaf param_max_features  \\\n",
       "28                        2                       2               auto   \n",
       "13                        7                       2               auto   \n",
       "0                         6                       3               auto   \n",
       "12                        5                       2               auto   \n",
       "24                        9                       3               auto   \n",
       "3                         6                       3               auto   \n",
       "22                        3                       3               auto   \n",
       "16                        2                       5               auto   \n",
       "26                        3                       4               auto   \n",
       "27                        6                       2               sqrt   \n",
       "17                        2                       3               sqrt   \n",
       "2                         5                       2               sqrt   \n",
       "8                         9                       4               sqrt   \n",
       "21                        9                       2               sqrt   \n",
       "18                        9                       3               sqrt   \n",
       "23                        7                       5               auto   \n",
       "29                        6                       3               sqrt   \n",
       "10                        7                       4               sqrt   \n",
       "6                         3                       2               sqrt   \n",
       "20                        4                       3               sqrt   \n",
       "9                         8                       3               sqrt   \n",
       "5                         6                       4               sqrt   \n",
       "25                        9                       4               sqrt   \n",
       "15                        9                       3               sqrt   \n",
       "1                         4                       5               sqrt   \n",
       "19                        9                       4               sqrt   \n",
       "4                         2                       5               sqrt   \n",
       "11                        8                       4               sqrt   \n",
       "7                         4                       4               sqrt   \n",
       "14                        8                       3               sqrt   \n",
       "\n",
       "    param_max_depth                                             params  \\\n",
       "28              NaN  {'min_samples_split': 2, 'min_samples_leaf': 2...   \n",
       "13             40.0  {'min_samples_split': 7, 'min_samples_leaf': 2...   \n",
       "0              40.0  {'min_samples_split': 6, 'min_samples_leaf': 3...   \n",
       "12             30.0  {'min_samples_split': 5, 'min_samples_leaf': 2...   \n",
       "24              NaN  {'min_samples_split': 9, 'min_samples_leaf': 3...   \n",
       "3              50.0  {'min_samples_split': 6, 'min_samples_leaf': 3...   \n",
       "22              NaN  {'min_samples_split': 3, 'min_samples_leaf': 3...   \n",
       "16             20.0  {'min_samples_split': 2, 'min_samples_leaf': 5...   \n",
       "26             20.0  {'min_samples_split': 3, 'min_samples_leaf': 4...   \n",
       "27             50.0  {'min_samples_split': 6, 'min_samples_leaf': 2...   \n",
       "17             50.0  {'min_samples_split': 2, 'min_samples_leaf': 3...   \n",
       "2              50.0  {'min_samples_split': 5, 'min_samples_leaf': 2...   \n",
       "8              50.0  {'min_samples_split': 9, 'min_samples_leaf': 4...   \n",
       "21              NaN  {'min_samples_split': 9, 'min_samples_leaf': 2...   \n",
       "18             50.0  {'min_samples_split': 9, 'min_samples_leaf': 3...   \n",
       "23             10.0  {'min_samples_split': 7, 'min_samples_leaf': 5...   \n",
       "29             40.0  {'min_samples_split': 6, 'min_samples_leaf': 3...   \n",
       "10             30.0  {'min_samples_split': 7, 'min_samples_leaf': 4...   \n",
       "6               NaN  {'min_samples_split': 3, 'min_samples_leaf': 2...   \n",
       "20              NaN  {'min_samples_split': 4, 'min_samples_leaf': 3...   \n",
       "9              30.0  {'min_samples_split': 8, 'min_samples_leaf': 3...   \n",
       "5              50.0  {'min_samples_split': 6, 'min_samples_leaf': 4...   \n",
       "25              NaN  {'min_samples_split': 9, 'min_samples_leaf': 4...   \n",
       "15             20.0  {'min_samples_split': 9, 'min_samples_leaf': 3...   \n",
       "1              30.0  {'min_samples_split': 4, 'min_samples_leaf': 5...   \n",
       "19             30.0  {'min_samples_split': 9, 'min_samples_leaf': 4...   \n",
       "4               NaN  {'min_samples_split': 2, 'min_samples_leaf': 5...   \n",
       "11             10.0  {'min_samples_split': 8, 'min_samples_leaf': 4...   \n",
       "7              10.0  {'min_samples_split': 4, 'min_samples_leaf': 4...   \n",
       "14             10.0  {'min_samples_split': 8, 'min_samples_leaf': 3...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "28       -1001.236490       -1058.934057       -1058.854094   \n",
       "13       -1002.447422       -1063.889973       -1062.411694   \n",
       "0        -1009.901878       -1073.509704       -1049.872131   \n",
       "12        -991.921014       -1065.153861       -1069.536650   \n",
       "24       -1020.296021       -1083.006077       -1053.042873   \n",
       "3        -1004.743013       -1070.711113       -1078.344092   \n",
       "22       -1023.468863       -1067.751193       -1074.516531   \n",
       "16       -1020.639787       -1084.559108       -1072.668320   \n",
       "26       -1010.414084       -1085.083943       -1067.449167   \n",
       "27       -1240.155530       -1291.924793       -1295.844133   \n",
       "17       -1233.635181       -1289.931796       -1318.570861   \n",
       "2        -1273.613252       -1257.522394       -1328.496327   \n",
       "8        -1246.858989       -1288.766951       -1330.180755   \n",
       "21       -1255.531304       -1308.201702       -1316.357329   \n",
       "18       -1232.206354       -1323.099634       -1327.773148   \n",
       "23       -1263.202128       -1303.637086       -1296.203022   \n",
       "29       -1231.557140       -1294.157054       -1344.850336   \n",
       "10       -1231.467465       -1310.421398       -1353.717812   \n",
       "6        -1225.319166       -1305.919640       -1346.034638   \n",
       "20       -1226.940001       -1327.176307       -1297.349799   \n",
       "9        -1238.590213       -1333.160455       -1303.246706   \n",
       "5        -1223.932771       -1327.438985       -1323.655617   \n",
       "25       -1239.716483       -1323.031124       -1338.907060   \n",
       "15       -1219.649308       -1368.606596       -1339.048832   \n",
       "1        -1243.198180       -1344.860458       -1367.554583   \n",
       "19       -1292.142023       -1349.195267       -1326.605038   \n",
       "4        -1219.412034       -1421.238612       -1338.840104   \n",
       "11       -1588.587843       -1664.176200       -1682.263488   \n",
       "7        -1579.663306       -1660.021871       -1662.073398   \n",
       "14       -1613.077897       -1635.479698       -1719.187057   \n",
       "\n",
       "    split3_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "28       -1085.195997     -1051.055160       30.701779                1  \n",
       "13       -1076.725722     -1051.368703       28.788018                2  \n",
       "0        -1077.139125     -1052.605709       26.785981                3  \n",
       "12       -1083.942226     -1052.638438       35.737648                4  \n",
       "24       -1090.846133     -1061.797776       27.805769                5  \n",
       "3        -1094.151952     -1061.987542       34.114230                6  \n",
       "22       -1084.435310     -1062.542974       23.326766                7  \n",
       "16       -1085.179196     -1065.761602       26.523905                8  \n",
       "26       -1101.869405     -1066.204150       34.433046                9  \n",
       "27       -1313.993573     -1285.479507       27.460344               10  \n",
       "17       -1312.656963     -1288.698701       33.540483               11  \n",
       "2        -1299.736395     -1289.842092       26.925755               12  \n",
       "8        -1322.168002     -1296.993674       32.849257               13  \n",
       "21       -1310.796740     -1297.721769       24.536215               14  \n",
       "18       -1309.233393     -1298.078132       38.637419               15  \n",
       "23       -1330.821215     -1298.465863       24.094926               16  \n",
       "29       -1327.280501     -1299.461258       43.223785               17  \n",
       "10       -1306.039563     -1300.411559       43.950870               18  \n",
       "6        -1326.132472     -1300.851479       45.857014               19  \n",
       "20       -1361.604226     -1303.267583       49.587577               20  \n",
       "9        -1345.450902     -1305.112069       41.359915               21  \n",
       "5        -1349.931627     -1306.239750       48.569895               22  \n",
       "25       -1340.866300     -1310.630242       41.522105               23  \n",
       "15       -1326.119557     -1313.356073       56.250783               24  \n",
       "1        -1334.200795     -1322.453504       47.316974               25  \n",
       "19       -1343.903711     -1327.961510       22.304158               26  \n",
       "4        -1360.183488     -1334.918559       73.223296               27  \n",
       "11       -1632.921829     -1641.987340       35.525355               28  \n",
       "7        -1679.004942     -1645.190879       38.542935               29  \n",
       "14       -1634.533892     -1650.569636       40.616598               30  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.concat([pd.read_csv('models/xgb-random-grid-search-results-01.csv'), pd.read_csv('models/xgb-random-grid-search-results-02.csv')])\n",
    "df = pd.read_csv('models/rf-random-grid-search-results-cycle1.csv')\n",
    "df.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053aeecc-47bd-44ee-81d0-59fdc16b68d4",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655b5449-345b-460f-9ca7-4688db79ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:  2.6min finished\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "rf = RandomForestRegressor(\n",
    "    verbose=1, \n",
    "    n_estimators=100, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=2, \n",
    "    max_features='auto', \n",
    "    max_depth=None, \n",
    "    n_jobs=5)\n",
    "rf.fit(X_train, y_train)\n",
    "# with open('models/ensemble_rf.pkl', 'wb') as f:\n",
    "#     pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6495748-050e-4f9d-be82-abc54e79ea0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3473354f-67e4-4630-b427-f3b92a61de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (mean, std): 712.078 (26.255)\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoostingRegressor\n",
    "regr = HistGradientBoostingRegressor(max_iter=400, verbose=1, random_state=42)\n",
    "\n",
    "# cross validation\n",
    "scores = abs(cross_val_score(\n",
    "    regr, X_train, y_train, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    cv=KFold(n_splits=5), verbose=0, n_jobs=5))\n",
    "print(f'RMSE (mean, std): %.3f (%.3f)' % (scores.mean(), scores.std()))\n",
    "\n",
    "# store result\n",
    "results['HistGradientBoostingRegressor'] = [scores.mean(), scores.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3550864d-77aa-447f-b3c6-728e0c8ea130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean(RMSE)</th>\n",
       "      <th>std(RMSE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>712.078042</td>\n",
       "      <td>26.254768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>1671.421257</td>\n",
       "      <td>18.811115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mean(RMSE)  std(RMSE)\n",
       "HistGradientBoostingRegressor   712.078042  26.254768\n",
       "DecisionTreeRegressor          1671.421257  18.811115"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=['mean(RMSE)', 'std(RMSE)']).T.sort_values('mean(RMSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f60513-833a-468a-878e-2f995e6937a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.025 GB of training data: 0.807 s\n",
      "Binning 0.003 GB of validation data: 0.015 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/400] 1 tree, 31 leaves, max depth = 7, train loss: 12951961.20340, val loss: 12773424.58551, in 0.066s\n",
      "[2/400] 1 tree, 31 leaves, max depth = 7, train loss: 11019400.65969, val loss: 10842353.48396, in 0.067s\n",
      "[3/400] 1 tree, 31 leaves, max depth = 7, train loss: 9400572.47632, val loss: 9263000.56627, in 0.062s\n",
      "[4/400] 1 tree, 31 leaves, max depth = 7, train loss: 8075088.89619, val loss: 7960540.24420, in 0.175s\n",
      "[5/400] 1 tree, 31 leaves, max depth = 7, train loss: 6951721.90938, val loss: 6843062.04099, in 0.064s\n",
      "[6/400] 1 tree, 31 leaves, max depth = 7, train loss: 6014522.26140, val loss: 5933625.74559, in 0.063s\n",
      "[7/400] 1 tree, 31 leaves, max depth = 6, train loss: 5228719.49461, val loss: 5139983.75980, in 0.063s\n",
      "[8/400] 1 tree, 31 leaves, max depth = 7, train loss: 4574064.30700, val loss: 4488279.89753, in 0.068s\n",
      "[9/400] 1 tree, 31 leaves, max depth = 7, train loss: 4017008.18056, val loss: 3944925.39016, in 0.063s\n",
      "[10/400] 1 tree, 31 leaves, max depth = 7, train loss: 3551401.11053, val loss: 3493386.03755, in 0.066s\n",
      "[11/400] 1 tree, 31 leaves, max depth = 8, train loss: 3151442.28411, val loss: 3110867.95212, in 0.065s\n",
      "[12/400] 1 tree, 31 leaves, max depth = 7, train loss: 2812859.69010, val loss: 2781213.59789, in 0.070s\n",
      "[13/400] 1 tree, 31 leaves, max depth = 7, train loss: 2524155.44523, val loss: 2505796.07115, in 0.061s\n",
      "[14/400] 1 tree, 31 leaves, max depth = 7, train loss: 2279091.59605, val loss: 2274093.99192, in 0.065s\n",
      "[15/400] 1 tree, 31 leaves, max depth = 8, train loss: 2057500.11747, val loss: 2061600.68516, in 0.070s\n",
      "[16/400] 1 tree, 31 leaves, max depth = 7, train loss: 1870330.96543, val loss: 1879982.80373, in 0.065s\n",
      "[17/400] 1 tree, 31 leaves, max depth = 8, train loss: 1708076.11044, val loss: 1731113.31060, in 0.063s\n",
      "[18/400] 1 tree, 31 leaves, max depth = 7, train loss: 1564931.36576, val loss: 1598337.93137, in 0.063s\n",
      "[19/400] 1 tree, 31 leaves, max depth = 10, train loss: 1435393.55047, val loss: 1473726.86376, in 0.064s\n",
      "[20/400] 1 tree, 31 leaves, max depth = 8, train loss: 1326172.61452, val loss: 1371447.32231, in 0.057s\n",
      "[21/400] 1 tree, 31 leaves, max depth = 8, train loss: 1230733.87402, val loss: 1282060.71031, in 0.058s\n",
      "[22/400] 1 tree, 31 leaves, max depth = 7, train loss: 1140548.79540, val loss: 1194930.33944, in 0.060s\n",
      "[23/400] 1 tree, 31 leaves, max depth = 11, train loss: 1060673.37443, val loss: 1122686.45167, in 0.059s\n",
      "[24/400] 1 tree, 31 leaves, max depth = 7, train loss: 989901.21228, val loss: 1056860.39883, in 0.061s\n",
      "[25/400] 1 tree, 31 leaves, max depth = 9, train loss: 926177.58003, val loss: 996897.34661, in 0.064s\n",
      "[26/400] 1 tree, 31 leaves, max depth = 7, train loss: 872864.11525, val loss: 946458.06781, in 0.063s\n",
      "[27/400] 1 tree, 31 leaves, max depth = 9, train loss: 820588.34375, val loss: 896846.35434, in 0.066s\n",
      "[28/400] 1 tree, 31 leaves, max depth = 8, train loss: 773736.84833, val loss: 854310.68852, in 0.062s\n",
      "[29/400] 1 tree, 31 leaves, max depth = 9, train loss: 731719.06986, val loss: 818455.04124, in 0.064s\n",
      "[30/400] 1 tree, 31 leaves, max depth = 9, train loss: 694208.69474, val loss: 783626.95627, in 0.062s\n",
      "[31/400] 1 tree, 31 leaves, max depth = 10, train loss: 661035.11382, val loss: 752822.77004, in 0.063s\n",
      "[32/400] 1 tree, 31 leaves, max depth = 10, train loss: 629553.00361, val loss: 721489.04242, in 0.060s\n",
      "[33/400] 1 tree, 31 leaves, max depth = 8, train loss: 603447.85766, val loss: 694496.88130, in 0.069s\n",
      "[34/400] 1 tree, 31 leaves, max depth = 7, train loss: 574703.79340, val loss: 667146.06626, in 0.067s\n",
      "[35/400] 1 tree, 31 leaves, max depth = 9, train loss: 551204.66593, val loss: 645252.86571, in 0.069s\n",
      "[36/400] 1 tree, 31 leaves, max depth = 8, train loss: 527485.79384, val loss: 623235.24891, in 0.061s\n",
      "[37/400] 1 tree, 31 leaves, max depth = 12, train loss: 505358.37882, val loss: 603851.81234, in 0.066s\n",
      "[38/400] 1 tree, 31 leaves, max depth = 9, train loss: 485504.83150, val loss: 586094.34248, in 0.061s\n",
      "[39/400] 1 tree, 31 leaves, max depth = 10, train loss: 468039.24378, val loss: 570439.58204, in 0.066s\n",
      "[40/400] 1 tree, 31 leaves, max depth = 10, train loss: 451918.74526, val loss: 556573.46946, in 0.062s\n",
      "[41/400] 1 tree, 31 leaves, max depth = 9, train loss: 436197.44521, val loss: 539784.83910, in 0.063s\n",
      "[42/400] 1 tree, 31 leaves, max depth = 9, train loss: 422959.00404, val loss: 528426.88612, in 0.059s\n",
      "[43/400] 1 tree, 31 leaves, max depth = 10, train loss: 409135.80652, val loss: 515184.12622, in 0.058s\n",
      "[44/400] 1 tree, 31 leaves, max depth = 10, train loss: 397809.86915, val loss: 504349.38537, in 0.057s\n",
      "[45/400] 1 tree, 31 leaves, max depth = 10, train loss: 385648.34382, val loss: 492868.55651, in 0.060s\n",
      "[46/400] 1 tree, 31 leaves, max depth = 11, train loss: 375323.34202, val loss: 484977.62030, in 0.064s\n",
      "[47/400] 1 tree, 31 leaves, max depth = 9, train loss: 366153.97758, val loss: 475511.14781, in 0.068s\n",
      "[48/400] 1 tree, 31 leaves, max depth = 10, train loss: 356257.98328, val loss: 467278.65662, in 0.062s\n",
      "[49/400] 1 tree, 31 leaves, max depth = 9, train loss: 348427.64951, val loss: 460058.04712, in 0.065s\n",
      "[50/400] 1 tree, 31 leaves, max depth = 10, train loss: 341143.74639, val loss: 453001.93505, in 0.063s\n",
      "[51/400] 1 tree, 31 leaves, max depth = 9, train loss: 333265.31305, val loss: 445475.54584, in 0.062s\n",
      "[52/400] 1 tree, 31 leaves, max depth = 11, train loss: 325912.55394, val loss: 436910.90174, in 0.059s\n",
      "[53/400] 1 tree, 31 leaves, max depth = 10, train loss: 319911.73288, val loss: 431275.33745, in 0.059s\n",
      "[54/400] 1 tree, 31 leaves, max depth = 9, train loss: 314195.23757, val loss: 425054.91929, in 0.062s\n",
      "[55/400] 1 tree, 31 leaves, max depth = 10, train loss: 308816.05061, val loss: 420261.33163, in 0.062s\n",
      "[56/400] 1 tree, 31 leaves, max depth = 9, train loss: 303391.74568, val loss: 414937.23154, in 0.063s\n",
      "[57/400] 1 tree, 31 leaves, max depth = 12, train loss: 297656.37874, val loss: 407969.97462, in 0.059s\n",
      "[58/400] 1 tree, 31 leaves, max depth = 10, train loss: 293562.48850, val loss: 404514.95540, in 0.057s\n",
      "[59/400] 1 tree, 31 leaves, max depth = 12, train loss: 288816.48375, val loss: 400540.29557, in 0.061s\n",
      "[60/400] 1 tree, 31 leaves, max depth = 11, train loss: 283074.66137, val loss: 395053.61365, in 0.054s\n",
      "[61/400] 1 tree, 31 leaves, max depth = 10, train loss: 279011.43223, val loss: 391224.45129, in 0.062s\n",
      "[62/400] 1 tree, 31 leaves, max depth = 12, train loss: 274743.92069, val loss: 387004.28118, in 0.063s\n",
      "[63/400] 1 tree, 31 leaves, max depth = 13, train loss: 270568.64301, val loss: 383200.44127, in 0.058s\n",
      "[64/400] 1 tree, 31 leaves, max depth = 9, train loss: 265978.28670, val loss: 378559.30079, in 0.066s\n",
      "[65/400] 1 tree, 31 leaves, max depth = 9, train loss: 262197.61010, val loss: 374524.86599, in 0.061s\n",
      "[66/400] 1 tree, 31 leaves, max depth = 10, train loss: 258586.80070, val loss: 370932.54298, in 0.060s\n",
      "[67/400] 1 tree, 31 leaves, max depth = 10, train loss: 255608.95884, val loss: 367795.14141, in 0.061s\n",
      "[68/400] 1 tree, 31 leaves, max depth = 13, train loss: 251086.31792, val loss: 363141.86754, in 0.058s\n",
      "[69/400] 1 tree, 31 leaves, max depth = 9, train loss: 248316.24882, val loss: 360704.17835, in 0.058s\n",
      "[70/400] 1 tree, 31 leaves, max depth = 10, train loss: 245552.59922, val loss: 359050.72191, in 0.059s\n",
      "[71/400] 1 tree, 31 leaves, max depth = 13, train loss: 242772.90449, val loss: 356143.46539, in 0.055s\n",
      "[72/400] 1 tree, 31 leaves, max depth = 10, train loss: 240065.72107, val loss: 354083.72060, in 0.062s\n",
      "[73/400] 1 tree, 31 leaves, max depth = 9, train loss: 237611.09301, val loss: 351633.93861, in 0.058s\n",
      "[74/400] 1 tree, 31 leaves, max depth = 10, train loss: 234902.42975, val loss: 349304.86912, in 0.056s\n",
      "[75/400] 1 tree, 31 leaves, max depth = 14, train loss: 232247.23257, val loss: 346930.15714, in 0.072s\n",
      "[76/400] 1 tree, 31 leaves, max depth = 14, train loss: 229718.46225, val loss: 344894.36074, in 0.055s\n",
      "[77/400] 1 tree, 31 leaves, max depth = 10, train loss: 227483.36937, val loss: 342253.74711, in 0.058s\n",
      "[78/400] 1 tree, 31 leaves, max depth = 11, train loss: 225072.54591, val loss: 339927.29422, in 0.057s\n",
      "[79/400] 1 tree, 31 leaves, max depth = 11, train loss: 223003.67143, val loss: 337628.64008, in 0.057s\n",
      "[80/400] 1 tree, 31 leaves, max depth = 10, train loss: 220658.45203, val loss: 335223.63870, in 0.059s\n",
      "[81/400] 1 tree, 31 leaves, max depth = 13, train loss: 218755.10184, val loss: 333120.03448, in 0.061s\n",
      "[82/400] 1 tree, 31 leaves, max depth = 9, train loss: 217137.92098, val loss: 331723.10351, in 0.056s\n",
      "[83/400] 1 tree, 31 leaves, max depth = 11, train loss: 215192.49794, val loss: 329509.24591, in 0.058s\n",
      "[84/400] 1 tree, 31 leaves, max depth = 11, train loss: 213116.10365, val loss: 328404.74378, in 0.063s\n",
      "[85/400] 1 tree, 31 leaves, max depth = 10, train loss: 211623.16233, val loss: 326969.90057, in 0.062s\n",
      "[86/400] 1 tree, 31 leaves, max depth = 16, train loss: 209747.88568, val loss: 325041.46832, in 0.058s\n",
      "[87/400] 1 tree, 31 leaves, max depth = 12, train loss: 208172.24026, val loss: 322734.72367, in 0.058s\n",
      "[88/400] 1 tree, 31 leaves, max depth = 12, train loss: 206814.22488, val loss: 321509.11367, in 0.059s\n",
      "[89/400] 1 tree, 31 leaves, max depth = 10, train loss: 205426.27341, val loss: 320727.07305, in 0.062s\n",
      "[90/400] 1 tree, 31 leaves, max depth = 10, train loss: 204227.75257, val loss: 319223.24107, in 0.055s\n",
      "[91/400] 1 tree, 31 leaves, max depth = 9, train loss: 202574.06635, val loss: 318566.23650, in 0.059s\n",
      "[92/400] 1 tree, 31 leaves, max depth = 9, train loss: 201179.14643, val loss: 317144.74600, in 0.063s\n",
      "[93/400] 1 tree, 31 leaves, max depth = 10, train loss: 199213.70466, val loss: 314317.05419, in 0.055s\n",
      "[94/400] 1 tree, 31 leaves, max depth = 8, train loss: 197945.54141, val loss: 312708.13020, in 0.062s\n",
      "[95/400] 1 tree, 31 leaves, max depth = 12, train loss: 196572.87594, val loss: 311426.19145, in 0.064s\n",
      "[96/400] 1 tree, 31 leaves, max depth = 13, train loss: 195103.27301, val loss: 309580.40204, in 0.057s\n",
      "[97/400] 1 tree, 31 leaves, max depth = 12, train loss: 193905.90534, val loss: 308652.03497, in 0.057s\n",
      "[98/400] 1 tree, 31 leaves, max depth = 11, train loss: 192460.45609, val loss: 307335.81638, in 0.060s\n",
      "[99/400] 1 tree, 31 leaves, max depth = 9, train loss: 191423.43123, val loss: 306761.72195, in 0.057s\n",
      "[100/400] 1 tree, 31 leaves, max depth = 10, train loss: 190176.37699, val loss: 306115.76858, in 0.057s\n",
      "[101/400] 1 tree, 31 leaves, max depth = 13, train loss: 189205.18850, val loss: 305405.82518, in 0.060s\n",
      "[102/400] 1 tree, 31 leaves, max depth = 7, train loss: 187193.26496, val loss: 303208.80160, in 0.052s\n",
      "[103/400] 1 tree, 31 leaves, max depth = 11, train loss: 185916.03346, val loss: 302936.10520, in 0.058s\n",
      "[104/400] 1 tree, 31 leaves, max depth = 8, train loss: 184673.20983, val loss: 301230.95248, in 0.056s\n",
      "[105/400] 1 tree, 31 leaves, max depth = 10, train loss: 183543.55852, val loss: 300492.28856, in 0.060s\n",
      "[106/400] 1 tree, 31 leaves, max depth = 11, train loss: 182585.58602, val loss: 300233.02677, in 0.060s\n",
      "[107/400] 1 tree, 31 leaves, max depth = 15, train loss: 181778.10418, val loss: 299083.16739, in 0.054s\n",
      "[108/400] 1 tree, 31 leaves, max depth = 10, train loss: 181041.76600, val loss: 298504.25596, in 0.059s\n",
      "[109/400] 1 tree, 31 leaves, max depth = 8, train loss: 179628.17396, val loss: 297515.24836, in 0.053s\n",
      "[110/400] 1 tree, 31 leaves, max depth = 12, train loss: 178882.56801, val loss: 297034.83926, in 0.051s\n",
      "[111/400] 1 tree, 31 leaves, max depth = 10, train loss: 177058.91983, val loss: 295022.21786, in 0.062s\n",
      "[112/400] 1 tree, 31 leaves, max depth = 9, train loss: 176135.07923, val loss: 294396.44850, in 0.059s\n",
      "[113/400] 1 tree, 31 leaves, max depth = 10, train loss: 175356.16006, val loss: 294003.54131, in 0.061s\n",
      "[114/400] 1 tree, 31 leaves, max depth = 13, train loss: 174347.43537, val loss: 292933.05357, in 0.062s\n",
      "[115/400] 1 tree, 31 leaves, max depth = 11, train loss: 173666.85829, val loss: 292604.82575, in 0.058s\n",
      "[116/400] 1 tree, 31 leaves, max depth = 10, train loss: 172655.80119, val loss: 292356.10863, in 0.051s\n",
      "[117/400] 1 tree, 31 leaves, max depth = 13, train loss: 171545.34559, val loss: 291255.47774, in 0.061s\n",
      "[118/400] 1 tree, 31 leaves, max depth = 9, train loss: 170062.51476, val loss: 289203.91537, in 0.054s\n",
      "[119/400] 1 tree, 31 leaves, max depth = 12, train loss: 169082.04385, val loss: 288307.66753, in 0.060s\n",
      "[120/400] 1 tree, 31 leaves, max depth = 11, train loss: 168461.59376, val loss: 288086.45727, in 0.061s\n",
      "[121/400] 1 tree, 31 leaves, max depth = 11, train loss: 167706.13802, val loss: 287761.13018, in 0.055s\n",
      "[122/400] 1 tree, 31 leaves, max depth = 8, train loss: 167062.95871, val loss: 287461.04787, in 0.058s\n",
      "[123/400] 1 tree, 31 leaves, max depth = 11, train loss: 166211.94960, val loss: 287198.63249, in 0.059s\n",
      "[124/400] 1 tree, 31 leaves, max depth = 11, train loss: 165297.19347, val loss: 286399.99817, in 0.056s\n",
      "[125/400] 1 tree, 31 leaves, max depth = 11, train loss: 164699.21478, val loss: 285893.82352, in 0.061s\n",
      "[126/400] 1 tree, 31 leaves, max depth = 12, train loss: 163927.02875, val loss: 285568.66087, in 0.054s\n",
      "[127/400] 1 tree, 31 leaves, max depth = 12, train loss: 163043.73501, val loss: 284635.86001, in 0.056s\n",
      "[128/400] 1 tree, 31 leaves, max depth = 11, train loss: 162258.70252, val loss: 283863.44371, in 0.058s\n",
      "[129/400] 1 tree, 31 leaves, max depth = 10, train loss: 161751.58645, val loss: 283638.98352, in 0.057s\n",
      "[130/400] 1 tree, 31 leaves, max depth = 9, train loss: 160985.35304, val loss: 283050.78603, in 0.060s\n",
      "[131/400] 1 tree, 31 leaves, max depth = 12, train loss: 160175.64593, val loss: 282617.10342, in 0.062s\n",
      "[132/400] 1 tree, 31 leaves, max depth = 12, train loss: 159483.02749, val loss: 282094.31327, in 0.058s\n",
      "[133/400] 1 tree, 31 leaves, max depth = 12, train loss: 158804.78588, val loss: 281899.22014, in 0.063s\n",
      "[134/400] 1 tree, 31 leaves, max depth = 9, train loss: 158307.16565, val loss: 281694.77046, in 0.062s\n",
      "[135/400] 1 tree, 31 leaves, max depth = 10, train loss: 157530.46847, val loss: 280746.44609, in 0.059s\n",
      "[136/400] 1 tree, 31 leaves, max depth = 9, train loss: 156850.56029, val loss: 280046.61753, in 0.061s\n",
      "[137/400] 1 tree, 31 leaves, max depth = 14, train loss: 156201.17583, val loss: 280057.68232, in 0.058s\n",
      "[138/400] 1 tree, 31 leaves, max depth = 8, train loss: 155699.17614, val loss: 279903.71865, in 0.057s\n",
      "[139/400] 1 tree, 31 leaves, max depth = 10, train loss: 154691.28122, val loss: 278698.54852, in 0.056s\n",
      "[140/400] 1 tree, 31 leaves, max depth = 12, train loss: 154150.98333, val loss: 278322.85597, in 0.058s\n",
      "[141/400] 1 tree, 31 leaves, max depth = 9, train loss: 153672.37005, val loss: 277991.88959, in 0.057s\n",
      "[142/400] 1 tree, 31 leaves, max depth = 11, train loss: 153066.14353, val loss: 277710.84250, in 0.054s\n",
      "[143/400] 1 tree, 31 leaves, max depth = 10, train loss: 152356.27008, val loss: 276853.90914, in 0.059s\n",
      "[144/400] 1 tree, 31 leaves, max depth = 11, train loss: 151752.06572, val loss: 276167.31209, in 0.049s\n",
      "[145/400] 1 tree, 31 leaves, max depth = 10, train loss: 151039.23787, val loss: 275437.14300, in 0.060s\n",
      "[146/400] 1 tree, 31 leaves, max depth = 9, train loss: 150486.58181, val loss: 275045.79994, in 0.057s\n",
      "[147/400] 1 tree, 31 leaves, max depth = 10, train loss: 149829.66067, val loss: 274314.87626, in 0.061s\n",
      "[148/400] 1 tree, 31 leaves, max depth = 11, train loss: 149317.02132, val loss: 274297.05829, in 0.057s\n",
      "[149/400] 1 tree, 31 leaves, max depth = 13, train loss: 148889.10710, val loss: 274181.12450, in 0.061s\n",
      "[150/400] 1 tree, 31 leaves, max depth = 10, train loss: 148279.88612, val loss: 273770.42575, in 0.057s\n",
      "[151/400] 1 tree, 31 leaves, max depth = 7, train loss: 147894.07383, val loss: 273739.13665, in 0.053s\n",
      "[152/400] 1 tree, 31 leaves, max depth = 11, train loss: 147389.60949, val loss: 273177.11527, in 0.057s\n",
      "[153/400] 1 tree, 31 leaves, max depth = 9, train loss: 146919.20010, val loss: 273191.01612, in 0.056s\n",
      "[154/400] 1 tree, 31 leaves, max depth = 9, train loss: 146315.62741, val loss: 272731.52326, in 0.054s\n",
      "[155/400] 1 tree, 31 leaves, max depth = 8, train loss: 145716.79095, val loss: 272013.94231, in 0.059s\n",
      "[156/400] 1 tree, 31 leaves, max depth = 9, train loss: 145013.79674, val loss: 271258.71016, in 0.055s\n",
      "[157/400] 1 tree, 31 leaves, max depth = 13, train loss: 144216.19741, val loss: 270349.77695, in 0.061s\n",
      "[158/400] 1 tree, 31 leaves, max depth = 13, train loss: 143718.16692, val loss: 270280.29504, in 0.057s\n",
      "[159/400] 1 tree, 31 leaves, max depth = 13, train loss: 143300.25865, val loss: 270024.16485, in 0.062s\n",
      "[160/400] 1 tree, 31 leaves, max depth = 8, train loss: 142978.39143, val loss: 269734.02438, in 0.053s\n",
      "[161/400] 1 tree, 31 leaves, max depth = 12, train loss: 142555.11670, val loss: 269684.05482, in 0.053s\n",
      "[162/400] 1 tree, 31 leaves, max depth = 10, train loss: 142141.22332, val loss: 269382.44389, in 0.055s\n",
      "[163/400] 1 tree, 31 leaves, max depth = 9, train loss: 141705.18338, val loss: 269020.47469, in 0.058s\n",
      "[164/400] 1 tree, 31 leaves, max depth = 12, train loss: 141155.53188, val loss: 268254.63738, in 0.059s\n",
      "[165/400] 1 tree, 31 leaves, max depth = 9, train loss: 140629.12389, val loss: 268052.65918, in 0.060s\n",
      "[166/400] 1 tree, 31 leaves, max depth = 12, train loss: 140183.01593, val loss: 267898.67711, in 0.058s\n",
      "[167/400] 1 tree, 31 leaves, max depth = 10, train loss: 139493.29502, val loss: 267089.13919, in 0.059s\n",
      "[168/400] 1 tree, 31 leaves, max depth = 10, train loss: 139028.09356, val loss: 266713.27692, in 0.054s\n",
      "[169/400] 1 tree, 31 leaves, max depth = 18, train loss: 138500.90304, val loss: 266377.32276, in 0.060s\n",
      "[170/400] 1 tree, 31 leaves, max depth = 9, train loss: 138043.42606, val loss: 266047.66495, in 0.055s\n",
      "[171/400] 1 tree, 31 leaves, max depth = 16, train loss: 137610.31005, val loss: 265751.97320, in 0.061s\n",
      "[172/400] 1 tree, 31 leaves, max depth = 12, train loss: 136955.96995, val loss: 264454.31721, in 0.056s\n",
      "[173/400] 1 tree, 31 leaves, max depth = 10, train loss: 136607.56652, val loss: 264421.09408, in 0.057s\n",
      "[174/400] 1 tree, 31 leaves, max depth = 10, train loss: 136171.51838, val loss: 264116.28894, in 0.058s\n",
      "[175/400] 1 tree, 31 leaves, max depth = 11, train loss: 135806.41389, val loss: 263947.98413, in 0.063s\n",
      "[176/400] 1 tree, 31 leaves, max depth = 17, train loss: 135275.97809, val loss: 263544.37498, in 0.057s\n",
      "[177/400] 1 tree, 31 leaves, max depth = 11, train loss: 134841.94495, val loss: 263336.09786, in 0.060s\n",
      "[178/400] 1 tree, 31 leaves, max depth = 13, train loss: 134452.38590, val loss: 263227.79279, in 0.054s\n",
      "[179/400] 1 tree, 31 leaves, max depth = 10, train loss: 134011.16735, val loss: 262862.70955, in 0.049s\n",
      "[180/400] 1 tree, 31 leaves, max depth = 10, train loss: 133719.65754, val loss: 262840.19053, in 0.059s\n",
      "[181/400] 1 tree, 31 leaves, max depth = 9, train loss: 133382.93544, val loss: 262682.64648, in 0.057s\n",
      "[182/400] 1 tree, 31 leaves, max depth = 9, train loss: 133062.47731, val loss: 262439.61274, in 0.055s\n",
      "[183/400] 1 tree, 31 leaves, max depth = 10, train loss: 132553.43672, val loss: 262109.71302, in 0.060s\n",
      "[184/400] 1 tree, 31 leaves, max depth = 9, train loss: 131957.93475, val loss: 261615.47963, in 0.059s\n",
      "[185/400] 1 tree, 31 leaves, max depth = 12, train loss: 131486.23626, val loss: 261091.56387, in 0.055s\n",
      "[186/400] 1 tree, 31 leaves, max depth = 11, train loss: 131111.59837, val loss: 260816.61444, in 0.055s\n",
      "[187/400] 1 tree, 31 leaves, max depth = 12, train loss: 130722.87594, val loss: 260406.92788, in 0.053s\n",
      "[188/400] 1 tree, 31 leaves, max depth = 14, train loss: 130374.43803, val loss: 260207.35886, in 0.049s\n",
      "[189/400] 1 tree, 31 leaves, max depth = 14, train loss: 129962.30629, val loss: 259897.28258, in 0.059s\n",
      "[190/400] 1 tree, 31 leaves, max depth = 9, train loss: 129510.22344, val loss: 259681.54875, in 0.047s\n",
      "[191/400] 1 tree, 31 leaves, max depth = 10, train loss: 129120.01429, val loss: 259641.30563, in 0.052s\n",
      "[192/400] 1 tree, 31 leaves, max depth = 9, train loss: 128734.01887, val loss: 259262.35051, in 0.053s\n",
      "[193/400] 1 tree, 31 leaves, max depth = 12, train loss: 128377.29579, val loss: 259380.61151, in 0.052s\n",
      "[194/400] 1 tree, 31 leaves, max depth = 10, train loss: 128088.84586, val loss: 259255.27217, in 0.047s\n",
      "[195/400] 1 tree, 31 leaves, max depth = 16, train loss: 127699.67560, val loss: 259162.62852, in 0.051s\n",
      "[196/400] 1 tree, 31 leaves, max depth = 10, train loss: 127335.23411, val loss: 258844.64689, in 0.054s\n",
      "[197/400] 1 tree, 31 leaves, max depth = 9, train loss: 127036.33518, val loss: 258722.79519, in 0.058s\n",
      "[198/400] 1 tree, 31 leaves, max depth = 12, train loss: 126736.26654, val loss: 258789.64614, in 0.053s\n",
      "[199/400] 1 tree, 31 leaves, max depth = 8, train loss: 126462.05028, val loss: 258726.95159, in 0.061s\n",
      "[200/400] 1 tree, 31 leaves, max depth = 9, train loss: 126087.05883, val loss: 258463.17171, in 0.054s\n",
      "[201/400] 1 tree, 31 leaves, max depth = 9, train loss: 125784.49267, val loss: 258418.53965, in 0.054s\n",
      "[202/400] 1 tree, 31 leaves, max depth = 14, train loss: 125279.49636, val loss: 257794.57569, in 0.052s\n",
      "[203/400] 1 tree, 31 leaves, max depth = 9, train loss: 124934.58376, val loss: 257476.39085, in 0.058s\n",
      "[204/400] 1 tree, 31 leaves, max depth = 10, train loss: 124599.57001, val loss: 257438.73595, in 0.059s\n",
      "[205/400] 1 tree, 31 leaves, max depth = 9, train loss: 124306.91191, val loss: 257316.92523, in 0.055s\n",
      "[206/400] 1 tree, 31 leaves, max depth = 9, train loss: 123928.17127, val loss: 256863.47782, in 0.055s\n",
      "[207/400] 1 tree, 31 leaves, max depth = 15, train loss: 123649.80535, val loss: 256788.88529, in 0.058s\n",
      "[208/400] 1 tree, 31 leaves, max depth = 9, train loss: 123327.25124, val loss: 256754.61257, in 0.050s\n",
      "[209/400] 1 tree, 31 leaves, max depth = 9, train loss: 123020.77469, val loss: 256816.95179, in 0.058s\n",
      "[210/400] 1 tree, 31 leaves, max depth = 12, train loss: 122769.72245, val loss: 256311.59575, in 0.056s\n",
      "[211/400] 1 tree, 31 leaves, max depth = 9, train loss: 122414.68816, val loss: 256077.06322, in 0.062s\n",
      "[212/400] 1 tree, 31 leaves, max depth = 9, train loss: 122062.92156, val loss: 255740.67528, in 0.064s\n",
      "[213/400] 1 tree, 31 leaves, max depth = 13, train loss: 121794.26169, val loss: 255451.60887, in 0.058s\n",
      "[214/400] 1 tree, 31 leaves, max depth = 8, train loss: 121483.51764, val loss: 255411.86181, in 0.053s\n",
      "[215/400] 1 tree, 31 leaves, max depth = 12, train loss: 121172.75193, val loss: 255411.57699, in 0.049s\n",
      "[216/400] 1 tree, 31 leaves, max depth = 10, train loss: 120884.93908, val loss: 255043.22323, in 0.056s\n",
      "[217/400] 1 tree, 31 leaves, max depth = 10, train loss: 120652.93025, val loss: 254848.28779, in 0.057s\n",
      "[218/400] 1 tree, 31 leaves, max depth = 11, train loss: 120434.82210, val loss: 254777.84990, in 0.054s\n",
      "[219/400] 1 tree, 31 leaves, max depth = 13, train loss: 120048.98113, val loss: 254611.00002, in 0.051s\n",
      "[220/400] 1 tree, 31 leaves, max depth = 11, train loss: 119667.86228, val loss: 254535.10162, in 0.048s\n",
      "[221/400] 1 tree, 31 leaves, max depth = 12, train loss: 119444.28416, val loss: 254589.19517, in 0.051s\n",
      "[222/400] 1 tree, 31 leaves, max depth = 13, train loss: 119049.74179, val loss: 254222.40227, in 0.056s\n",
      "[223/400] 1 tree, 31 leaves, max depth = 12, train loss: 118706.14318, val loss: 254074.05676, in 0.058s\n",
      "[224/400] 1 tree, 31 leaves, max depth = 12, train loss: 118379.19637, val loss: 253783.35139, in 0.063s\n",
      "[225/400] 1 tree, 31 leaves, max depth = 12, train loss: 118101.03757, val loss: 253417.65047, in 0.055s\n",
      "[226/400] 1 tree, 31 leaves, max depth = 12, train loss: 117827.35465, val loss: 253112.51512, in 0.056s\n",
      "[227/400] 1 tree, 31 leaves, max depth = 12, train loss: 117545.19225, val loss: 252938.38239, in 0.045s\n",
      "[228/400] 1 tree, 31 leaves, max depth = 9, train loss: 117189.51417, val loss: 252604.95109, in 0.055s\n",
      "[229/400] 1 tree, 31 leaves, max depth = 11, train loss: 116902.42635, val loss: 252379.81451, in 0.049s\n",
      "[230/400] 1 tree, 31 leaves, max depth = 9, train loss: 116642.59386, val loss: 252159.07399, in 0.057s\n",
      "[231/400] 1 tree, 31 leaves, max depth = 10, train loss: 116293.22690, val loss: 251907.97515, in 0.063s\n",
      "[232/400] 1 tree, 31 leaves, max depth = 9, train loss: 116068.56044, val loss: 251592.04483, in 0.053s\n",
      "[233/400] 1 tree, 31 leaves, max depth = 10, train loss: 115713.72531, val loss: 251356.01897, in 0.048s\n",
      "[234/400] 1 tree, 31 leaves, max depth = 9, train loss: 115481.36237, val loss: 251311.91649, in 0.053s\n",
      "[235/400] 1 tree, 31 leaves, max depth = 12, train loss: 115231.31335, val loss: 251377.66539, in 0.048s\n",
      "[236/400] 1 tree, 31 leaves, max depth = 14, train loss: 114997.03890, val loss: 251147.20968, in 0.054s\n",
      "[237/400] 1 tree, 31 leaves, max depth = 12, train loss: 114747.60310, val loss: 251036.44543, in 0.056s\n",
      "[238/400] 1 tree, 31 leaves, max depth = 14, train loss: 114542.96349, val loss: 250876.19922, in 0.053s\n",
      "[239/400] 1 tree, 31 leaves, max depth = 12, train loss: 114081.06795, val loss: 250177.90027, in 0.059s\n",
      "[240/400] 1 tree, 31 leaves, max depth = 13, train loss: 113732.41334, val loss: 250029.28733, in 0.051s\n",
      "[241/400] 1 tree, 31 leaves, max depth = 10, train loss: 113386.58539, val loss: 250012.08434, in 0.060s\n",
      "[242/400] 1 tree, 31 leaves, max depth = 9, train loss: 113141.51948, val loss: 249836.23372, in 0.052s\n",
      "[243/400] 1 tree, 31 leaves, max depth = 10, train loss: 112897.67112, val loss: 249761.85587, in 0.053s\n",
      "[244/400] 1 tree, 31 leaves, max depth = 10, train loss: 112589.55582, val loss: 249498.39707, in 0.058s\n",
      "[245/400] 1 tree, 31 leaves, max depth = 12, train loss: 112344.69641, val loss: 249388.94562, in 0.056s\n",
      "[246/400] 1 tree, 31 leaves, max depth = 12, train loss: 112130.80554, val loss: 249486.73130, in 0.054s\n",
      "[247/400] 1 tree, 31 leaves, max depth = 11, train loss: 111835.02032, val loss: 249216.65452, in 0.056s\n",
      "[248/400] 1 tree, 31 leaves, max depth = 12, train loss: 111552.47878, val loss: 249159.99768, in 0.059s\n",
      "[249/400] 1 tree, 31 leaves, max depth = 9, train loss: 111338.39010, val loss: 249101.79946, in 0.062s\n",
      "[250/400] 1 tree, 31 leaves, max depth = 11, train loss: 111081.93804, val loss: 248838.33074, in 0.056s\n",
      "[251/400] 1 tree, 31 leaves, max depth = 9, train loss: 110723.01705, val loss: 248398.09493, in 0.063s\n",
      "[252/400] 1 tree, 31 leaves, max depth = 10, train loss: 110537.58122, val loss: 248376.36138, in 0.054s\n",
      "[253/400] 1 tree, 31 leaves, max depth = 10, train loss: 110295.06726, val loss: 248213.78724, in 0.055s\n",
      "[254/400] 1 tree, 31 leaves, max depth = 9, train loss: 109957.18897, val loss: 247975.75384, in 0.052s\n",
      "[255/400] 1 tree, 31 leaves, max depth = 10, train loss: 109756.54060, val loss: 247904.32780, in 0.060s\n",
      "[256/400] 1 tree, 31 leaves, max depth = 13, train loss: 109548.88906, val loss: 247922.86059, in 0.054s\n",
      "[257/400] 1 tree, 31 leaves, max depth = 11, train loss: 109269.57430, val loss: 247780.55562, in 0.054s\n",
      "[258/400] 1 tree, 31 leaves, max depth = 12, train loss: 109074.14433, val loss: 247922.78868, in 0.051s\n",
      "[259/400] 1 tree, 31 leaves, max depth = 13, train loss: 108841.37349, val loss: 247805.34093, in 0.055s\n",
      "[260/400] 1 tree, 31 leaves, max depth = 14, train loss: 108534.25597, val loss: 247519.70097, in 0.056s\n",
      "[261/400] 1 tree, 31 leaves, max depth = 11, train loss: 108026.15238, val loss: 247020.00449, in 0.057s\n",
      "[262/400] 1 tree, 31 leaves, max depth = 9, train loss: 107796.03275, val loss: 247032.14954, in 0.061s\n",
      "[263/400] 1 tree, 31 leaves, max depth = 8, train loss: 107632.99504, val loss: 246825.95415, in 0.051s\n",
      "[264/400] 1 tree, 31 leaves, max depth = 12, train loss: 107311.57464, val loss: 246487.60605, in 0.053s\n",
      "[265/400] 1 tree, 31 leaves, max depth = 9, train loss: 107166.69225, val loss: 246427.38775, in 0.049s\n",
      "[266/400] 1 tree, 31 leaves, max depth = 11, train loss: 107034.75450, val loss: 246377.06973, in 0.052s\n",
      "[267/400] 1 tree, 31 leaves, max depth = 13, train loss: 106789.93294, val loss: 246259.38906, in 0.061s\n",
      "[268/400] 1 tree, 31 leaves, max depth = 12, train loss: 106572.09586, val loss: 246065.52437, in 0.059s\n",
      "[269/400] 1 tree, 31 leaves, max depth = 19, train loss: 106281.41615, val loss: 245878.68613, in 0.058s\n",
      "[270/400] 1 tree, 31 leaves, max depth = 14, train loss: 105890.56078, val loss: 245293.43937, in 0.049s\n",
      "[271/400] 1 tree, 31 leaves, max depth = 10, train loss: 105693.28608, val loss: 245302.04990, in 0.048s\n",
      "[272/400] 1 tree, 31 leaves, max depth = 10, train loss: 105547.66286, val loss: 245307.41936, in 0.060s\n",
      "[273/400] 1 tree, 31 leaves, max depth = 10, train loss: 105254.04155, val loss: 245108.21190, in 0.060s\n",
      "[274/400] 1 tree, 31 leaves, max depth = 11, train loss: 105030.39344, val loss: 244953.79640, in 0.062s\n",
      "[275/400] 1 tree, 31 leaves, max depth = 13, train loss: 104751.43344, val loss: 244830.14726, in 0.060s\n",
      "[276/400] 1 tree, 31 leaves, max depth = 13, train loss: 104480.53172, val loss: 244868.27699, in 0.051s\n",
      "[277/400] 1 tree, 31 leaves, max depth = 10, train loss: 104287.20062, val loss: 244783.00897, in 0.059s\n",
      "[278/400] 1 tree, 31 leaves, max depth = 10, train loss: 104139.34330, val loss: 245019.01244, in 0.046s\n",
      "[279/400] 1 tree, 31 leaves, max depth = 12, train loss: 103792.40425, val loss: 244844.36880, in 0.051s\n",
      "[280/400] 1 tree, 31 leaves, max depth = 11, train loss: 103540.60548, val loss: 244801.26391, in 0.054s\n",
      "[281/400] 1 tree, 31 leaves, max depth = 11, train loss: 103159.07976, val loss: 244248.44015, in 0.058s\n",
      "[282/400] 1 tree, 31 leaves, max depth = 11, train loss: 102939.59494, val loss: 244488.83207, in 0.059s\n",
      "[283/400] 1 tree, 31 leaves, max depth = 14, train loss: 102780.50528, val loss: 244490.51858, in 0.048s\n",
      "[284/400] 1 tree, 31 leaves, max depth = 12, train loss: 102551.43422, val loss: 244293.56538, in 0.048s\n",
      "[285/400] 1 tree, 31 leaves, max depth = 11, train loss: 102416.94299, val loss: 244201.42610, in 0.047s\n",
      "[286/400] 1 tree, 31 leaves, max depth = 10, train loss: 102237.22488, val loss: 244190.81437, in 0.053s\n",
      "[287/400] 1 tree, 31 leaves, max depth = 11, train loss: 102033.82333, val loss: 244211.44491, in 0.048s\n",
      "[288/400] 1 tree, 31 leaves, max depth = 11, train loss: 101770.17250, val loss: 244050.42192, in 0.063s\n",
      "[289/400] 1 tree, 31 leaves, max depth = 10, train loss: 101495.72424, val loss: 243784.02370, in 0.052s\n",
      "[290/400] 1 tree, 31 leaves, max depth = 13, train loss: 101295.85217, val loss: 243754.53081, in 0.047s\n",
      "[291/400] 1 tree, 31 leaves, max depth = 8, train loss: 101112.37619, val loss: 243532.06345, in 0.051s\n",
      "[292/400] 1 tree, 31 leaves, max depth = 14, train loss: 100932.62770, val loss: 243503.31430, in 0.049s\n",
      "[293/400] 1 tree, 31 leaves, max depth = 12, train loss: 100707.69436, val loss: 243406.69658, in 0.048s\n",
      "[294/400] 1 tree, 31 leaves, max depth = 12, train loss: 100491.31499, val loss: 243258.52563, in 0.046s\n",
      "[295/400] 1 tree, 31 leaves, max depth = 9, train loss: 100324.79960, val loss: 243160.54539, in 0.055s\n",
      "[296/400] 1 tree, 31 leaves, max depth = 10, train loss: 100110.20805, val loss: 243207.92336, in 0.049s\n",
      "[297/400] 1 tree, 31 leaves, max depth = 12, train loss: 99885.53571, val loss: 243082.30765, in 0.050s\n",
      "[298/400] 1 tree, 31 leaves, max depth = 12, train loss: 99652.86026, val loss: 242958.29358, in 0.057s\n",
      "[299/400] 1 tree, 31 leaves, max depth = 12, train loss: 99522.73215, val loss: 242998.76835, in 0.051s\n",
      "[300/400] 1 tree, 31 leaves, max depth = 11, train loss: 99294.94451, val loss: 242885.82997, in 0.045s\n",
      "[301/400] 1 tree, 31 leaves, max depth = 13, train loss: 99061.17071, val loss: 242561.59535, in 0.043s\n",
      "[302/400] 1 tree, 31 leaves, max depth = 12, train loss: 98952.84822, val loss: 242520.42278, in 0.050s\n",
      "[303/400] 1 tree, 31 leaves, max depth = 9, train loss: 98739.91566, val loss: 242404.01948, in 0.051s\n",
      "[304/400] 1 tree, 31 leaves, max depth = 13, train loss: 98613.90327, val loss: 242461.75868, in 0.049s\n",
      "[305/400] 1 tree, 31 leaves, max depth = 11, train loss: 98446.92888, val loss: 242348.70706, in 0.049s\n",
      "[306/400] 1 tree, 31 leaves, max depth = 16, train loss: 98233.00202, val loss: 242303.50021, in 0.048s\n",
      "[307/400] 1 tree, 31 leaves, max depth = 12, train loss: 97995.38463, val loss: 242347.85077, in 0.055s\n",
      "[308/400] 1 tree, 31 leaves, max depth = 10, train loss: 97832.73094, val loss: 242394.62277, in 0.051s\n",
      "[309/400] 1 tree, 31 leaves, max depth = 13, train loss: 97581.53208, val loss: 242205.81245, in 0.044s\n",
      "[310/400] 1 tree, 31 leaves, max depth = 14, train loss: 97401.90758, val loss: 242129.80111, in 0.051s\n",
      "[311/400] 1 tree, 31 leaves, max depth = 10, train loss: 97249.92344, val loss: 242000.55802, in 0.050s\n",
      "[312/400] 1 tree, 31 leaves, max depth = 10, train loss: 97020.97162, val loss: 241832.69115, in 0.050s\n",
      "[313/400] 1 tree, 31 leaves, max depth = 11, train loss: 96791.00661, val loss: 241545.24995, in 0.047s\n",
      "[314/400] 1 tree, 31 leaves, max depth = 11, train loss: 96702.82774, val loss: 241568.60943, in 0.047s\n",
      "[315/400] 1 tree, 31 leaves, max depth = 13, train loss: 96538.89202, val loss: 241497.86421, in 0.057s\n",
      "[316/400] 1 tree, 31 leaves, max depth = 13, train loss: 96332.19843, val loss: 241415.52458, in 0.051s\n",
      "[317/400] 1 tree, 31 leaves, max depth = 11, train loss: 96132.62477, val loss: 241301.69116, in 0.055s\n",
      "[318/400] 1 tree, 31 leaves, max depth = 15, train loss: 95898.48275, val loss: 241180.41265, in 0.053s\n",
      "[319/400] 1 tree, 31 leaves, max depth = 9, train loss: 95673.41069, val loss: 240973.40741, in 0.060s\n",
      "[320/400] 1 tree, 31 leaves, max depth = 11, train loss: 95500.85279, val loss: 240941.57067, in 0.050s\n",
      "[321/400] 1 tree, 31 leaves, max depth = 9, train loss: 95309.14999, val loss: 240992.08171, in 0.054s\n",
      "[322/400] 1 tree, 31 leaves, max depth = 10, train loss: 95171.41681, val loss: 240853.23040, in 0.051s\n",
      "[323/400] 1 tree, 31 leaves, max depth = 9, train loss: 95000.25458, val loss: 240558.16078, in 0.046s\n",
      "[324/400] 1 tree, 31 leaves, max depth = 15, train loss: 94862.54818, val loss: 240748.05173, in 0.050s\n",
      "[325/400] 1 tree, 31 leaves, max depth = 9, train loss: 94737.85764, val loss: 240679.77233, in 0.047s\n",
      "[326/400] 1 tree, 31 leaves, max depth = 10, train loss: 94602.79421, val loss: 240567.87651, in 0.051s\n",
      "[327/400] 1 tree, 31 leaves, max depth = 10, train loss: 94443.37629, val loss: 240776.68723, in 0.053s\n",
      "[328/400] 1 tree, 31 leaves, max depth = 9, train loss: 94233.59249, val loss: 240719.87425, in 0.062s\n",
      "[329/400] 1 tree, 31 leaves, max depth = 12, train loss: 94057.97937, val loss: 240590.52535, in 0.057s\n",
      "[330/400] 1 tree, 31 leaves, max depth = 12, train loss: 93929.47136, val loss: 240560.47150, in 0.059s\n",
      "[331/400] 1 tree, 31 leaves, max depth = 10, train loss: 93697.28484, val loss: 240432.20507, in 0.144s\n",
      "[332/400] 1 tree, 31 leaves, max depth = 12, train loss: 93544.38507, val loss: 240293.08131, in 0.046s\n",
      "[333/400] 1 tree, 31 leaves, max depth = 10, train loss: 93284.36146, val loss: 240021.86356, in 0.056s\n",
      "[334/400] 1 tree, 31 leaves, max depth = 13, train loss: 93033.74890, val loss: 239692.67887, in 0.055s\n",
      "[335/400] 1 tree, 31 leaves, max depth = 12, train loss: 92816.96449, val loss: 239474.81985, in 0.057s\n",
      "[336/400] 1 tree, 31 leaves, max depth = 9, train loss: 92672.34471, val loss: 239472.51895, in 0.059s\n",
      "[337/400] 1 tree, 31 leaves, max depth = 9, train loss: 92479.56080, val loss: 239554.31462, in 0.059s\n",
      "[338/400] 1 tree, 31 leaves, max depth = 9, train loss: 92353.73242, val loss: 239459.18546, in 0.052s\n",
      "[339/400] 1 tree, 31 leaves, max depth = 14, train loss: 92161.85409, val loss: 239555.35419, in 0.050s\n",
      "[340/400] 1 tree, 31 leaves, max depth = 12, train loss: 91979.88239, val loss: 239545.58947, in 0.057s\n",
      "[341/400] 1 tree, 31 leaves, max depth = 10, train loss: 91796.66381, val loss: 239335.59929, in 0.066s\n",
      "[342/400] 1 tree, 31 leaves, max depth = 10, train loss: 91659.79059, val loss: 239269.81771, in 0.063s\n",
      "[343/400] 1 tree, 31 leaves, max depth = 10, train loss: 91536.51131, val loss: 239254.49881, in 0.058s\n",
      "[344/400] 1 tree, 31 leaves, max depth = 10, train loss: 91374.40030, val loss: 239080.51067, in 0.059s\n",
      "[345/400] 1 tree, 31 leaves, max depth = 17, train loss: 91183.24440, val loss: 238929.75020, in 0.057s\n",
      "[346/400] 1 tree, 31 leaves, max depth = 14, train loss: 91031.42951, val loss: 238837.98319, in 0.054s\n",
      "[347/400] 1 tree, 31 leaves, max depth = 12, train loss: 90913.67726, val loss: 238848.30918, in 0.049s\n",
      "[348/400] 1 tree, 31 leaves, max depth = 10, train loss: 90694.56387, val loss: 238618.16079, in 0.058s\n",
      "[349/400] 1 tree, 31 leaves, max depth = 11, train loss: 90526.76744, val loss: 238584.24334, in 0.053s\n",
      "[350/400] 1 tree, 31 leaves, max depth = 11, train loss: 90359.87254, val loss: 238666.26760, in 0.062s\n",
      "[351/400] 1 tree, 31 leaves, max depth = 10, train loss: 90210.53932, val loss: 238771.36152, in 0.051s\n",
      "[352/400] 1 tree, 31 leaves, max depth = 7, train loss: 90034.70104, val loss: 238708.91454, in 0.062s\n",
      "[353/400] 1 tree, 31 leaves, max depth = 10, train loss: 89782.77460, val loss: 238619.17354, in 0.062s\n",
      "[354/400] 1 tree, 31 leaves, max depth = 10, train loss: 89637.15417, val loss: 238587.86573, in 0.059s\n",
      "[355/400] 1 tree, 31 leaves, max depth = 10, train loss: 89506.74457, val loss: 238553.52811, in 0.050s\n",
      "[356/400] 1 tree, 31 leaves, max depth = 12, train loss: 89315.03563, val loss: 238409.55837, in 0.061s\n",
      "[357/400] 1 tree, 31 leaves, max depth = 9, train loss: 89181.33098, val loss: 238345.61132, in 0.047s\n",
      "[358/400] 1 tree, 31 leaves, max depth = 13, train loss: 89039.95824, val loss: 238288.54786, in 0.056s\n",
      "[359/400] 1 tree, 31 leaves, max depth = 8, train loss: 88964.92876, val loss: 238297.73043, in 0.042s\n",
      "[360/400] 1 tree, 31 leaves, max depth = 9, train loss: 88855.78886, val loss: 238285.16870, in 0.047s\n",
      "[361/400] 1 tree, 31 leaves, max depth = 10, train loss: 88743.41582, val loss: 238287.58906, in 0.054s\n",
      "[362/400] 1 tree, 31 leaves, max depth = 11, train loss: 88579.36894, val loss: 238261.87609, in 0.092s\n",
      "[363/400] 1 tree, 31 leaves, max depth = 9, train loss: 88440.71465, val loss: 238165.84128, in 0.053s\n",
      "[364/400] 1 tree, 31 leaves, max depth = 12, train loss: 88250.46490, val loss: 238121.03482, in 0.048s\n",
      "[365/400] 1 tree, 31 leaves, max depth = 10, train loss: 88169.90004, val loss: 238070.73984, in 0.045s\n",
      "[366/400] 1 tree, 31 leaves, max depth = 11, train loss: 87987.18158, val loss: 237927.86316, in 0.050s\n",
      "[367/400] 1 tree, 31 leaves, max depth = 13, train loss: 87820.03911, val loss: 237754.67268, in 0.061s\n",
      "[368/400] 1 tree, 31 leaves, max depth = 11, train loss: 87674.22035, val loss: 237676.91393, in 0.081s\n",
      "[369/400] 1 tree, 31 leaves, max depth = 14, train loss: 87566.39852, val loss: 237638.78706, in 0.055s\n",
      "[370/400] 1 tree, 31 leaves, max depth = 11, train loss: 87415.93231, val loss: 237505.97878, in 0.052s\n",
      "[371/400] 1 tree, 31 leaves, max depth = 11, train loss: 87279.33125, val loss: 237381.61645, in 0.073s\n",
      "[372/400] 1 tree, 31 leaves, max depth = 10, train loss: 87114.16132, val loss: 237332.83573, in 0.049s\n",
      "[373/400] 1 tree, 31 leaves, max depth = 14, train loss: 87020.60808, val loss: 237256.55335, in 0.054s\n",
      "[374/400] 1 tree, 31 leaves, max depth = 11, train loss: 86831.73518, val loss: 237148.49113, in 0.068s\n",
      "[375/400] 1 tree, 31 leaves, max depth = 17, train loss: 86719.79754, val loss: 237230.51099, in 0.060s\n",
      "[376/400] 1 tree, 31 leaves, max depth = 10, train loss: 86601.87797, val loss: 237249.63919, in 0.063s\n",
      "[377/400] 1 tree, 31 leaves, max depth = 11, train loss: 86465.24596, val loss: 237104.20556, in 0.057s\n",
      "[378/400] 1 tree, 31 leaves, max depth = 12, train loss: 86341.62001, val loss: 237072.35826, in 0.060s\n",
      "[379/400] 1 tree, 31 leaves, max depth = 10, train loss: 86243.22771, val loss: 236989.24413, in 0.048s\n",
      "[380/400] 1 tree, 31 leaves, max depth = 9, train loss: 86116.48563, val loss: 236942.47600, in 0.047s\n",
      "[381/400] 1 tree, 31 leaves, max depth = 14, train loss: 85984.45455, val loss: 236949.36855, in 0.048s\n",
      "[382/400] 1 tree, 31 leaves, max depth = 13, train loss: 85837.01481, val loss: 236850.67873, in 0.049s\n",
      "[383/400] 1 tree, 31 leaves, max depth = 16, train loss: 85771.93816, val loss: 236826.07877, in 0.049s\n",
      "[384/400] 1 tree, 31 leaves, max depth = 12, train loss: 85546.36275, val loss: 236659.65975, in 0.053s\n",
      "[385/400] 1 tree, 31 leaves, max depth = 9, train loss: 85412.26177, val loss: 236560.18354, in 0.058s\n",
      "[386/400] 1 tree, 31 leaves, max depth = 10, train loss: 85299.90752, val loss: 236525.18772, in 0.050s\n",
      "[387/400] 1 tree, 31 leaves, max depth = 9, train loss: 85222.62675, val loss: 236477.75863, in 0.049s\n",
      "[388/400] 1 tree, 31 leaves, max depth = 9, train loss: 85123.98856, val loss: 236464.76257, in 0.052s\n",
      "[389/400] 1 tree, 31 leaves, max depth = 9, train loss: 85056.88148, val loss: 236337.05478, in 0.048s\n",
      "[390/400] 1 tree, 31 leaves, max depth = 12, train loss: 84959.23670, val loss: 236351.82654, in 0.051s\n",
      "[391/400] 1 tree, 31 leaves, max depth = 10, train loss: 84875.61167, val loss: 236167.73048, in 0.053s\n",
      "[392/400] 1 tree, 31 leaves, max depth = 11, train loss: 84773.84074, val loss: 236113.33425, in 0.044s\n",
      "[393/400] 1 tree, 31 leaves, max depth = 17, train loss: 84556.82247, val loss: 235971.81028, in 0.048s\n",
      "[394/400] 1 tree, 31 leaves, max depth = 12, train loss: 84471.06770, val loss: 235994.09010, in 0.054s\n",
      "[395/400] 1 tree, 31 leaves, max depth = 8, train loss: 84402.41401, val loss: 236002.97897, in 0.047s\n",
      "[396/400] 1 tree, 31 leaves, max depth = 12, train loss: 84242.94854, val loss: 235847.24663, in 0.052s\n",
      "[397/400] 1 tree, 31 leaves, max depth = 10, train loss: 84127.70541, val loss: 235681.88072, in 0.049s\n",
      "[398/400] 1 tree, 31 leaves, max depth = 10, train loss: 84023.29954, val loss: 235590.06226, in 0.055s\n",
      "[399/400] 1 tree, 31 leaves, max depth = 11, train loss: 83888.98241, val loss: 235422.85267, in 0.059s\n",
      "[400/400] 1 tree, 31 leaves, max depth = 11, train loss: 83749.50744, val loss: 235395.75967, in 0.055s\n",
      "Fit 400 trees in 23.953 s, (12400 total leaves)\n",
      "Time spent computing histograms: 6.758s\n",
      "Time spent finding best splits:  2.549s\n",
      "Time spent applying splits:      2.670s\n",
      "Time spent predicting:           0.160s\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "regr.fit(X_train, y_train)\n",
    "with open('models/ensemble_gradient_boosting.pkl', 'wb') as f:\n",
    "    pickle.dump(regr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1aec925-4937-4280-b646-32697b150538",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'squared_error',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_iter': 400,\n",
       " 'max_leaf_nodes': 31,\n",
       " 'max_depth': None,\n",
       " 'min_samples_leaf': 20,\n",
       " 'l2_regularization': 0.0,\n",
       " 'max_bins': 255,\n",
       " 'monotonic_cst': None,\n",
       " 'categorical_features': None,\n",
       " 'warm_start': False,\n",
       " 'early_stopping': 'auto',\n",
       " 'scoring': 'loss',\n",
       " 'validation_fraction': 0.1,\n",
       " 'n_iter_no_change': 10,\n",
       " 'tol': 1e-07,\n",
       " 'verbose': 1,\n",
       " 'random_state': 42,\n",
       " 'feature_names_in_': array(['NumCust', 'AreaRoot', 'Perimeter', 'SideRatio', 'CentDepot',\n",
       "        'CentCustAvg', 'CentCustStd', 'Dispersion', 'AvgFurthest',\n",
       "        'AvgNearest', 'DepCustAvg', 'DepCustStd', 'DepCustMin',\n",
       "        'DepCustMed', 'DepCustMax', 'IntCustLinks', 'IntCustAvg',\n",
       "        'IntCustStd', 'IntCustMin', 'IntCustMed', 'IntCustMax', 'CapRatio',\n",
       "        'NumVehMin', 'DemAvg', 'DemStd', 'DemMin', 'DemMed', 'DemMax',\n",
       "        'PossRounds', 'StAvg', 'StStd', 'StMin', 'StMed', 'StMax',\n",
       "        'TwShare', 'TwWidthAvg', 'TwWidthStd', 'TwWidthMin', 'TwWidthMed',\n",
       "        'TwCentAvg', 'TwCentStd', 'TwCentMin', 'TwCentMax'], dtype=object),\n",
       " 'n_features_in_': 43,\n",
       " 'n_trees_per_iteration_': 1,\n",
       " '_random_seed': 1608637542,\n",
       " '_n_features': 43,\n",
       " 'is_categorical_': None,\n",
       " '_loss': <sklearn.ensemble._hist_gradient_boosting.loss.LeastSquares at 0x1f5532ac250>,\n",
       " 'do_early_stopping_': True,\n",
       " '_use_validation_data': True,\n",
       " '_bin_mapper': _BinMapper(n_threads=8, random_state=1608637542),\n",
       " '_baseline_prediction': 7739.00220888889,\n",
       " '_predictors': [[<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4640>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4100>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4520>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a42b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a48b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a41c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a44f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a45b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4070>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a43a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a43d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4940>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4970>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a47f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a47c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4a30>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4ac0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4130>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4a90>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a49a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4850>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4bb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4b80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4c70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4460>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4be0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4df0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a49d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4ee0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4a00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4160>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4e20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e0520>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288980d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898250>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f55321d400>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898100>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288983a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288983d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288985b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898400>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898580>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288982b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288985e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288986d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898640>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898940>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f55321d460>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898070>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e08b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288989d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288989a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d88e50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898b50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288981f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898a00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898c10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898cd0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898ca0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898370>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898ee0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898d00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898790>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898ac0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898b20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898e20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c5e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c790>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898fa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898df0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7220>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7370>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b74f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7490>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b76a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7520>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e0550>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b76d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7040>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7100>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7850>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b75e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b79d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7790>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7820>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7a30>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7a60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7940>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c8b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7b50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7640>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532a2880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b74c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7d60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7d90>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7d00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b77f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7ac0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7fa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7e50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7be0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0220>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0130>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d88e80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0070>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d88fa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0160>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0430>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7ca0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532a28b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0370>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0040>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898a30>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0490>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0670>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a01f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e0820>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a04c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a03a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0760>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a09d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532cdfd0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0a60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0a30>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0af0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0520>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0c10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0730>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0940>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0850>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a09a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0e80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0eb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0d00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0e20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0580>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73040>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0ee0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73220>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0be0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7b20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e732e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d88f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e730d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d889a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e731c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898eb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e730a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4d90>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e735e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e735b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73640>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73760>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73580>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e734c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e737c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e732b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73670>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e734f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73970>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e736d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73100>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73c10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7ee0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e739d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73550>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73df0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73c70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73cd0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73a60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e738e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73eb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73dc0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73ee0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73b20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73be0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc130>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73f10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc100>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc160>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b78b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc550>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288b7e80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73ac0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc3a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc490>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc6a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc820>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc6d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc670>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc0a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bca90>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcaf0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcb20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc8b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc3d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcbe0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc5e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcd90>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcc70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcb80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcd00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc2e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bce80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bceb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcfa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcca0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc2b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcb50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc730>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcac0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78070>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d88fd0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e783a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78490>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4fa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e785b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e781f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e781c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e782e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78730>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78670>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e780d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c7f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78760>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78460>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e786a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78940>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e789a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0ca0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e782b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e785e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78bb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78ac0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78970>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78c70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78d60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78d00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e787f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78a60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78550>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78e20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78b50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78e80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78fd0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e880d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78eb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78b80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78a00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78df0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d936a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e882e0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88160>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88130>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e884c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88280>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5531f87c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4cf10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e0610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88220>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88400>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88490>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88760>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e880a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88550>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88070>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e886d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88370>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e73b80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88820>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e888b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e885b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88940>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88bb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88b20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88970>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88c40>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a0b80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88d30>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88310>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88580>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898f40>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88790>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88ca0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88f40>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88e80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88b80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e811c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88df0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c760>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88f70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4c6a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532a63d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532cdb80>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bc910>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a08b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcd30>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e810d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e06d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81430>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e0700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e812b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81100>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d93700>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e813d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81550>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528898a60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81400>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81250>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81610>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81670>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81850>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81790>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e814c0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288bcdc0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81880>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81520>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e819d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81970>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81af0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81b50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81c10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81a60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e816a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5288a4b20>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81a90>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81760>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81730>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81d00>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81f40>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81cd0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81e50>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81df0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81f10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81d60>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81eb0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e81fa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532cdc70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532a27f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e93190>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e93160>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f5532e0850>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f526d4cf70>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e931f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e88fa0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e930d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e93400>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e93040>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e933a0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e78c10>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e936d0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e93670>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e934f0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e935b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e932b0>],\n",
       "  [<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor at 0x1f528e93700>]],\n",
       " '_scorer': None,\n",
       " 'train_score_': array([-15306242.65134831, -12951961.20340176, -11019400.65969382,\n",
       "         -9400572.47631917,  -8075088.89619364,  -6951721.90938268,\n",
       "         -6014522.26139884,  -5228719.49460928,  -4574064.30700284,\n",
       "         -4017008.18056415,  -3551401.11053181,  -3151442.28410748,\n",
       "         -2812859.6901017 ,  -2524155.44522667,  -2279091.59604791,\n",
       "         -2057500.11746861,  -1870330.96542774,  -1708076.11044182,\n",
       "         -1564931.36575962,  -1435393.55047388,  -1326172.61451996,\n",
       "         -1230733.87401522,  -1140548.79539896,  -1060673.3744339 ,\n",
       "          -989901.21227745,   -926177.58002869,   -872864.1152532 ,\n",
       "          -820588.34375005,   -773736.84833469,   -731719.06986205,\n",
       "          -694208.69474195,   -661035.1138221 ,   -629553.00360522,\n",
       "          -603447.85766127,   -574703.79339896,   -551204.66592799,\n",
       "          -527485.79383936,   -505358.37882335,   -485504.83149825,\n",
       "          -468039.24377675,   -451918.74525612,   -436197.44520634,\n",
       "          -422959.00403564,   -409135.80652196,   -397809.86915143,\n",
       "          -385648.34382459,   -375323.34202143,   -366153.97757965,\n",
       "          -356257.98328448,   -348427.64950697,   -341143.74638823,\n",
       "          -333265.313053  ,   -325912.55394318,   -319911.73287861,\n",
       "          -314195.23757399,   -308816.05060785,   -303391.74568352,\n",
       "          -297656.37873739,   -293562.48850042,   -288816.4837507 ,\n",
       "          -283074.66137363,   -279011.43223299,   -274743.92069226,\n",
       "          -270568.64300578,   -265978.28670247,   -262197.61009537,\n",
       "          -258586.80069613,   -255608.95883532,   -251086.31792493,\n",
       "          -248316.24881997,   -245552.59922023,   -242772.90448735,\n",
       "          -240065.72107048,   -237611.09300656,   -234902.42974797,\n",
       "          -232247.23256598,   -229718.4622465 ,   -227483.36937077,\n",
       "          -225072.54590801,   -223003.67142674,   -220658.45202897,\n",
       "          -218755.10183751,   -217137.92098216,   -215192.49794394,\n",
       "          -213116.10364522,   -211623.16232884,   -209747.88567672,\n",
       "          -208172.24026468,   -206814.22488342,   -205426.27340885,\n",
       "          -204227.75256555,   -202574.06635495,   -201179.14642669,\n",
       "          -199213.70465528,   -197945.54141307,   -196572.87594094,\n",
       "          -195103.27300845,   -193905.90533734,   -192460.45608544,\n",
       "          -191423.43123394,   -190176.37698878,   -189205.18849668,\n",
       "          -187193.26496187,   -185916.03346255,   -184673.20983163,\n",
       "          -183543.55851942,   -182585.5860156 ,   -181778.10418009,\n",
       "          -181041.76599671,   -179628.17396283,   -178882.56800764,\n",
       "          -177058.91983276,   -176135.07923182,   -175356.16006336,\n",
       "          -174347.43536786,   -173666.85829305,   -172655.80119099,\n",
       "          -171545.3455861 ,   -170062.51475509,   -169082.04384608,\n",
       "          -168461.59376329,   -167706.13801632,   -167062.95871339,\n",
       "          -166211.94960474,   -165297.19347492,   -164699.21478182,\n",
       "          -163927.02875289,   -163043.73501146,   -162258.70252251,\n",
       "          -161751.58645434,   -160985.35304412,   -160175.64592775,\n",
       "          -159483.02748546,   -158804.785885  ,   -158307.16564906,\n",
       "          -157530.46847059,   -156850.56029181,   -156201.17582829,\n",
       "          -155699.17614451,   -154691.28121816,   -154150.98332869,\n",
       "          -153672.3700529 ,   -153066.14352944,   -152356.27007554,\n",
       "          -151752.06572308,   -151039.23787146,   -150486.58180876,\n",
       "          -149829.66066678,   -149317.02132091,   -148889.1070987 ,\n",
       "          -148279.88612315,   -147894.07383389,   -147389.60948817,\n",
       "          -146919.20010047,   -146315.62741407,   -145716.79095244,\n",
       "          -145013.79673913,   -144216.19740563,   -143718.16692077,\n",
       "          -143300.25864956,   -142978.39143492,   -142555.11669983,\n",
       "          -142141.22331859,   -141705.18338223,   -141155.53188111,\n",
       "          -140629.12389359,   -140183.01592939,   -139493.29502141,\n",
       "          -139028.0935616 ,   -138500.90303771,   -138043.42606473,\n",
       "          -137610.31005108,   -136955.96994902,   -136607.56651695,\n",
       "          -136171.51837713,   -135806.41389288,   -135275.9780876 ,\n",
       "          -134841.94494687,   -134452.3858968 ,   -134011.16735237,\n",
       "          -133719.65753803,   -133382.93544269,   -133062.47731321,\n",
       "          -132553.4367239 ,   -131957.93475295,   -131486.23625546,\n",
       "          -131111.59837235,   -130722.87594388,   -130374.43802761,\n",
       "          -129962.30628873,   -129510.22344223,   -129120.01428536,\n",
       "          -128734.01886598,   -128377.29579102,   -128088.84585606,\n",
       "          -127699.67559938,   -127335.23411015,   -127036.33518483,\n",
       "          -126736.26654003,   -126462.0502798 ,   -126087.05883181,\n",
       "          -125784.492666  ,   -125279.49636442,   -124934.58375573,\n",
       "          -124599.57000594,   -124306.91190861,   -123928.17126773,\n",
       "          -123649.80535303,   -123327.2512377 ,   -123020.77468962,\n",
       "          -122769.72245239,   -122414.68815944,   -122062.92156295,\n",
       "          -121794.2616911 ,   -121483.51763818,   -121172.75192722,\n",
       "          -120884.9390763 ,   -120652.93025441,   -120434.82209525,\n",
       "          -120048.98113072,   -119667.86228372,   -119444.28415967,\n",
       "          -119049.74179408,   -118706.14318278,   -118379.19636901,\n",
       "          -118101.03757277,   -117827.35465064,   -117545.19224865,\n",
       "          -117189.51416642,   -116902.42634675,   -116642.59385716,\n",
       "          -116293.22689949,   -116068.56044144,   -115713.72530529,\n",
       "          -115481.36237191,   -115231.31335373,   -114997.03889911,\n",
       "          -114747.60310316,   -114542.96348599,   -114081.06795259,\n",
       "          -113732.4133361 ,   -113386.58538823,   -113141.51948097,\n",
       "          -112897.67112217,   -112589.55582047,   -112344.69641329,\n",
       "          -112130.80554337,   -111835.02032461,   -111552.4787766 ,\n",
       "          -111338.39009725,   -111081.9380386 ,   -110723.01704994,\n",
       "          -110537.5812232 ,   -110295.06726275,   -109957.18896684,\n",
       "          -109756.54060111,   -109548.88905631,   -109269.57430097,\n",
       "          -109074.14432931,   -108841.37349355,   -108534.25596915,\n",
       "          -108026.15238179,   -107796.03274968,   -107632.99503685,\n",
       "          -107311.57464291,   -107166.69224751,   -107034.75450479,\n",
       "          -106789.93293959,   -106572.09586312,   -106281.41614834,\n",
       "          -105890.56078059,   -105693.28607627,   -105547.66286238,\n",
       "          -105254.04155225,   -105030.3934375 ,   -104751.433445  ,\n",
       "          -104480.53171752,   -104287.20061834,   -104139.34330363,\n",
       "          -103792.40425256,   -103540.60548436,   -103159.07976318,\n",
       "          -102939.59494032,   -102780.50527878,   -102551.4342199 ,\n",
       "          -102416.94298579,   -102237.22488056,   -102033.82333414,\n",
       "          -101770.17249591,   -101495.72423629,   -101295.85217368,\n",
       "          -101112.37618531,   -100932.62770373,   -100707.69436122,\n",
       "          -100491.31498817,   -100324.7995994 ,   -100110.20805262,\n",
       "           -99885.5357086 ,    -99652.86026298,    -99522.73215421,\n",
       "           -99294.94451079,    -99061.17071117,    -98952.84822093,\n",
       "           -98739.91565565,    -98613.90326651,    -98446.92888246,\n",
       "           -98233.00202416,    -97995.38462714,    -97832.73093607,\n",
       "           -97581.53207741,    -97401.90758276,    -97249.92344456,\n",
       "           -97020.97162102,    -96791.00660768,    -96702.82773961,\n",
       "           -96538.89201851,    -96332.19843454,    -96132.62477468,\n",
       "           -95898.48274802,    -95673.41068526,    -95500.85279232,\n",
       "           -95309.1499865 ,    -95171.4168099 ,    -95000.25457655,\n",
       "           -94862.54818486,    -94737.85763911,    -94602.79421325,\n",
       "           -94443.37629291,    -94233.59249271,    -94057.97936836,\n",
       "           -93929.47136178,    -93697.2848396 ,    -93544.38507046,\n",
       "           -93284.36145594,    -93033.74889748,    -92816.96448991,\n",
       "           -92672.34470591,    -92479.56080408,    -92353.73241579,\n",
       "           -92161.85408628,    -91979.88239003,    -91796.6638063 ,\n",
       "           -91659.79058821,    -91536.51131281,    -91374.40029583,\n",
       "           -91183.24440315,    -91031.42951486,    -90913.67726409,\n",
       "           -90694.56387066,    -90526.76744269,    -90359.87254267,\n",
       "           -90210.53931571,    -90034.70103704,    -89782.77460216,\n",
       "           -89637.15416758,    -89506.74457341,    -89315.03562658,\n",
       "           -89181.3309842 ,    -89039.95824273,    -88964.92875825,\n",
       "           -88855.78885666,    -88743.41582432,    -88579.36893996,\n",
       "           -88440.71465179,    -88250.46490331,    -88169.90003772,\n",
       "           -87987.18158392,    -87820.03911437,    -87674.22035339,\n",
       "           -87566.39851713,    -87415.93230689,    -87279.33124753,\n",
       "           -87114.16131635,    -87020.6080755 ,    -86831.73517842,\n",
       "           -86719.79753516,    -86601.87796878,    -86465.24596174,\n",
       "           -86341.62000888,    -86243.22771378,    -86116.48562887,\n",
       "           -85984.45455261,    -85837.01481381,    -85771.93815633,\n",
       "           -85546.36275123,    -85412.26177313,    -85299.90751932,\n",
       "           -85222.62674855,    -85123.98856022,    -85056.88147933,\n",
       "           -84959.23669901,    -84875.61167348,    -84773.84073648,\n",
       "           -84556.82247076,    -84471.06770333,    -84402.41401398,\n",
       "           -84242.94853688,    -84127.70540694,    -84023.29954161,\n",
       "           -83888.98240892,    -83749.50744345]),\n",
       " 'validation_score_': array([-15098924.27543417, -12773424.58551063, -10842353.48396388,\n",
       "         -9263000.56626783,  -7960540.24420311,  -6843062.0409893 ,\n",
       "         -5933625.74559067,  -5139983.75980323,  -4488279.89752767,\n",
       "         -3944925.39015812,  -3493386.03754644,  -3110867.95211684,\n",
       "         -2781213.59788626,  -2505796.07115106,  -2274093.99191964,\n",
       "         -2061600.68516262,  -1879982.80372502,  -1731113.31060442,\n",
       "         -1598337.93137437,  -1473726.86375949,  -1371447.32230716,\n",
       "         -1282060.71031012,  -1194930.3394447 ,  -1122686.45166722,\n",
       "         -1056860.39883025,   -996897.34660776,   -946458.06781279,\n",
       "          -896846.35434248,   -854310.68852399,   -818455.04123507,\n",
       "          -783626.95627056,   -752822.77003937,   -721489.04242272,\n",
       "          -694496.88129701,   -667146.06625703,   -645252.86571241,\n",
       "          -623235.24890892,   -603851.81233544,   -586094.34248216,\n",
       "          -570439.58203883,   -556573.46946194,   -539784.83910288,\n",
       "          -528426.88612342,   -515184.12621926,   -504349.38537203,\n",
       "          -492868.55651344,   -484977.62030074,   -475511.14781066,\n",
       "          -467278.65662464,   -460058.04711873,   -453001.93505248,\n",
       "          -445475.5458366 ,   -436910.90173978,   -431275.3374536 ,\n",
       "          -425054.91929078,   -420261.33163401,   -414937.23154026,\n",
       "          -407969.97461606,   -404514.95540492,   -400540.29556842,\n",
       "          -395053.61365277,   -391224.45129178,   -387004.28117855,\n",
       "          -383200.4412675 ,   -378559.3007898 ,   -374524.86599404,\n",
       "          -370932.54297898,   -367795.14140928,   -363141.86754443,\n",
       "          -360704.17835067,   -359050.72190615,   -356143.46538548,\n",
       "          -354083.7205974 ,   -351633.93860946,   -349304.86911689,\n",
       "          -346930.15714447,   -344894.36073913,   -342253.74711157,\n",
       "          -339927.29421566,   -337628.64008396,   -335223.63870147,\n",
       "          -333120.03448095,   -331723.10350974,   -329509.24590875,\n",
       "          -328404.74377682,   -326969.90057333,   -325041.46832191,\n",
       "          -322734.72367432,   -321509.11367489,   -320727.0730474 ,\n",
       "          -319223.24106691,   -318566.23650184,   -317144.74600241,\n",
       "          -314317.05419212,   -312708.13019791,   -311426.19144661,\n",
       "          -309580.40204446,   -308652.03497021,   -307335.816385  ,\n",
       "          -306761.72195021,   -306115.76857565,   -305405.82517847,\n",
       "          -303208.80160216,   -302936.10520429,   -301230.95248291,\n",
       "          -300492.28856216,   -300233.02677018,   -299083.16738541,\n",
       "          -298504.25595833,   -297515.24835855,   -297034.83926235,\n",
       "          -295022.21786165,   -294396.44849521,   -294003.54130881,\n",
       "          -292933.05357029,   -292604.82575086,   -292356.10863185,\n",
       "          -291255.47774435,   -289203.91537306,   -288307.66753321,\n",
       "          -288086.45727106,   -287761.13017523,   -287461.04786601,\n",
       "          -287198.63248815,   -286399.99817276,   -285893.82351943,\n",
       "          -285568.66086723,   -284635.86000587,   -283863.44370826,\n",
       "          -283638.98352015,   -283050.7860317 ,   -282617.10341577,\n",
       "          -282094.31326714,   -281899.22014025,   -281694.77045506,\n",
       "          -280746.44609418,   -280046.61752838,   -280057.68232178,\n",
       "          -279903.71865196,   -278698.54852299,   -278322.8559721 ,\n",
       "          -277991.88958806,   -277710.84249885,   -276853.90914435,\n",
       "          -276167.31208633,   -275437.14299794,   -275045.79994027,\n",
       "          -274314.87625886,   -274297.05829187,   -274181.12450031,\n",
       "          -273770.42574554,   -273739.13664621,   -273177.11527291,\n",
       "          -273191.01611958,   -272731.52325703,   -272013.9423091 ,\n",
       "          -271258.71016398,   -270349.7769453 ,   -270280.29504353,\n",
       "          -270024.16484867,   -269734.02438004,   -269684.05482363,\n",
       "          -269382.44389321,   -269020.47468855,   -268254.63737563,\n",
       "          -268052.65918069,   -267898.67710963,   -267089.13919389,\n",
       "          -266713.27691826,   -266377.32275773,   -266047.66495226,\n",
       "          -265751.97320165,   -264454.31720696,   -264421.09408248,\n",
       "          -264116.2889429 ,   -263947.98413308,   -263544.37497962,\n",
       "          -263336.09786262,   -263227.79279379,   -262862.7095536 ,\n",
       "          -262840.19052682,   -262682.64648417,   -262439.6127386 ,\n",
       "          -262109.71301987,   -261615.47963082,   -261091.56387194,\n",
       "          -260816.61443655,   -260406.9278803 ,   -260207.3588635 ,\n",
       "          -259897.28258113,   -259681.54875161,   -259641.30562926,\n",
       "          -259262.35051454,   -259380.61150681,   -259255.27217135,\n",
       "          -259162.62852295,   -258844.64689186,   -258722.79519117,\n",
       "          -258789.64613584,   -258726.95159441,   -258463.1717053 ,\n",
       "          -258418.53965326,   -257794.57569113,   -257476.3908466 ,\n",
       "          -257438.73594516,   -257316.92523387,   -256863.47781751,\n",
       "          -256788.88528507,   -256754.61257313,   -256816.95178998,\n",
       "          -256311.5957458 ,   -256077.0632182 ,   -255740.67528178,\n",
       "          -255451.60887245,   -255411.86180864,   -255411.576992  ,\n",
       "          -255043.22323473,   -254848.28779168,   -254777.84990457,\n",
       "          -254611.00002223,   -254535.10161757,   -254589.19516984,\n",
       "          -254222.40226992,   -254074.05675924,   -253783.3513899 ,\n",
       "          -253417.65046546,   -253112.51512128,   -252938.38238886,\n",
       "          -252604.9510868 ,   -252379.81451127,   -252159.07398894,\n",
       "          -251907.97515142,   -251592.04482959,   -251356.01896545,\n",
       "          -251311.91648836,   -251377.66538938,   -251147.20967789,\n",
       "          -251036.44543271,   -250876.19921974,   -250177.90026805,\n",
       "          -250029.28733105,   -250012.0843387 ,   -249836.23371932,\n",
       "          -249761.85587087,   -249498.39707294,   -249388.94561596,\n",
       "          -249486.73130364,   -249216.65451868,   -249159.99767755,\n",
       "          -249101.79945627,   -248838.33074449,   -248398.09493419,\n",
       "          -248376.36138352,   -248213.787244  ,   -247975.75384067,\n",
       "          -247904.32779507,   -247922.86058859,   -247780.55561515,\n",
       "          -247922.78868259,   -247805.34092554,   -247519.70097482,\n",
       "          -247020.00449395,   -247032.1495445 ,   -246825.95414575,\n",
       "          -246487.60605097,   -246427.38775105,   -246377.06972677,\n",
       "          -246259.38905684,   -246065.52436565,   -245878.68612587,\n",
       "          -245293.43936817,   -245302.04990222,   -245307.4193574 ,\n",
       "          -245108.21190062,   -244953.79639947,   -244830.14726444,\n",
       "          -244868.27699212,   -244783.00897436,   -245019.01243825,\n",
       "          -244844.36880141,   -244801.26390745,   -244248.44014932,\n",
       "          -244488.83207301,   -244490.51858212,   -244293.56537589,\n",
       "          -244201.42609867,   -244190.81437107,   -244211.44490896,\n",
       "          -244050.42191711,   -243784.02370389,   -243754.53081203,\n",
       "          -243532.06345243,   -243503.31429894,   -243406.6965845 ,\n",
       "          -243258.52563199,   -243160.54538706,   -243207.92335681,\n",
       "          -243082.30764907,   -242958.29358385,   -242998.76834905,\n",
       "          -242885.82997337,   -242561.59534895,   -242520.42278028,\n",
       "          -242404.01948479,   -242461.7586756 ,   -242348.70705533,\n",
       "          -242303.50020894,   -242347.85076997,   -242394.62277026,\n",
       "          -242205.81244937,   -242129.80110703,   -242000.55801734,\n",
       "          -241832.69115328,   -241545.24995306,   -241568.60943365,\n",
       "          -241497.86421374,   -241415.52457806,   -241301.69116128,\n",
       "          -241180.41265215,   -240973.40740636,   -240941.57066556,\n",
       "          -240992.08171059,   -240853.23039657,   -240558.1607755 ,\n",
       "          -240748.05172601,   -240679.77233446,   -240567.87651427,\n",
       "          -240776.68722891,   -240719.87424651,   -240590.52535283,\n",
       "          -240560.47149855,   -240432.20507489,   -240293.08130542,\n",
       "          -240021.86356365,   -239692.67886678,   -239474.81984997,\n",
       "          -239472.51895347,   -239554.31461715,   -239459.18545548,\n",
       "          -239555.35419085,   -239545.58946886,   -239335.59929387,\n",
       "          -239269.81771347,   -239254.4988133 ,   -239080.51066801,\n",
       "          -238929.75020282,   -238837.98318994,   -238848.30918436,\n",
       "          -238618.16079388,   -238584.24333874,   -238666.26759988,\n",
       "          -238771.36152198,   -238708.91453621,   -238619.17353657,\n",
       "          -238587.86573305,   -238553.52810916,   -238409.55837286,\n",
       "          -238345.61132279,   -238288.54785788,   -238297.73042719,\n",
       "          -238285.16869737,   -238287.58905513,   -238261.87609228,\n",
       "          -238165.84127684,   -238121.03482377,   -238070.73983713,\n",
       "          -237927.8631631 ,   -237754.67267959,   -237676.91392712,\n",
       "          -237638.78705586,   -237505.97878272,   -237381.61644841,\n",
       "          -237332.83573306,   -237256.55334987,   -237148.49112897,\n",
       "          -237230.51099408,   -237249.63919353,   -237104.20555846,\n",
       "          -237072.35826287,   -236989.2441291 ,   -236942.47600212,\n",
       "          -236949.36855193,   -236850.67872939,   -236826.07876826,\n",
       "          -236659.65974828,   -236560.18353821,   -236525.18771664,\n",
       "          -236477.75863048,   -236464.76256789,   -236337.05478223,\n",
       "          -236351.82653587,   -236167.73047844,   -236113.33425032,\n",
       "          -235971.81027998,   -235994.09009836,   -236002.9789705 ,\n",
       "          -235847.24662719,   -235681.88072388,   -235590.06225868,\n",
       "          -235422.85267109,   -235395.75966712])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969a11c-80cd-4c49-835c-1e63b89b2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [80, 90, 100, 110],\n",
    "#     'max_features': [2, 3],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [100, 200, 300, 1000]\n",
    "# }\n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [80],\n",
    "#     'max_features': [2],\n",
    "#     'min_samples_leaf': [3, 4],\n",
    "#     'min_samples_split': [8],\n",
    "#     'n_estimators': [100]\n",
    "# }\n",
    "# reg = HistGradientBoostingRegressor(verbose=3)\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     reg, \n",
    "#     param_grid, \n",
    "#     n_jobs=4, \n",
    "#     cv=3, \n",
    "#     verbose=4)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2650f-c5ab-4391-a84d-3b7dc139c5aa",
   "metadata": {},
   "source": [
    "## 4. Gradient boosting <a name=\"xgboost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7481500-c8b1-421c-8c90-ad52d8e63db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (mean, std): 829.728 (24.553)\n"
     ]
    }
   ],
   "source": [
    "# XGBRegressor\n",
    "regr = XGBRegressor(verbosity=2, random_state=42)\n",
    "\n",
    "# cross validation\n",
    "scores_xgb = abs(cross_val_score(\n",
    "    regr, X_train, y_train, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    cv=KFold(n_splits=5), verbose=0, n_jobs=5))\n",
    "print(f'RMSE (mean, std): %.3f (%.3f)' % (scores_xgb.mean(), scores_xgb.std()))\n",
    "\n",
    "# store result\n",
    "results['XGBRegressor'] = [scores_xgb.mean(), scores_xgb.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57daa85d-bd5d-4f30-8295-ee372a02b163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'max_depth': None,\n",
       " 'learning_rate': None,\n",
       " 'verbosity': 2,\n",
       " 'booster': None,\n",
       " 'tree_method': None,\n",
       " 'gamma': None,\n",
       " 'min_child_weight': None,\n",
       " 'max_delta_step': None,\n",
       " 'subsample': None,\n",
       " 'colsample_bytree': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'base_score': None,\n",
       " 'missing': nan,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': 42,\n",
       " 'n_jobs': None,\n",
       " 'monotone_constraints': None,\n",
       " 'interaction_constraints': None,\n",
       " 'importance_type': None,\n",
       " 'gpu_id': None,\n",
       " 'validate_parameters': None,\n",
       " 'predictor': None,\n",
       " 'enable_categorical': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fd16a6-0bea-4082-bbd3-3c53a9dbcaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean(RMSE)</th>\n",
       "      <th>std(RMSE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>829.727982</td>\n",
       "      <td>24.552532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean(RMSE)  std(RMSE)\n",
       "XGBRegressor  829.727982  24.552532"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=['mean(RMSE)', 'std(RMSE)']).T.sort_values('mean(RMSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33684e69-2636-45ca-9b39-a1c3b34a98c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:18:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 114 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:37] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:37] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 114 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 114 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 114 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:18:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "regr.fit(X_train, y_train)\n",
    "with open('models/ensemble_extreme_boosting.pkl', 'wb') as f:\n",
    "    pickle.dump(regr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf3cfa-e891-48de-90df-c34631c15c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Find n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8aafcb-4773-4289-bcad-782e811811e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 50, RMSE (mean, std): 873.084 (27.579)\n",
      "n_estimators: 100, RMSE (mean, std): 829.728 (24.553)\n",
      "n_estimators: 150, RMSE (mean, std): 812.690 (25.440)\n",
      "n_estimators: 200, RMSE (mean, std): 805.281 (26.048)\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 100, 150, 200, 300]:    # XGBRegressor\n",
    "    regr = XGBRegressor(verbosity=2, random_state=42, n_estimators = i)\n",
    "\n",
    "    # cross validation\n",
    "    scores_xgb = abs(cross_val_score(\n",
    "        regr, X_train, y_train, \n",
    "        scoring='neg_root_mean_squared_error', \n",
    "        cv=KFold(n_splits=5), verbose=0, n_jobs=5))\n",
    "    print(f'n_estimators: {i}, RMSE (mean, std): %.3f (%.3f)' % (scores_xgb.mean(), scores_xgb.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b27fc-0d19-490b-a074-217796ef14c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning cycle 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ee1f62-20cc-4558-aae5-0e3cf990ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.5, 0.3, 0.1, 0.05, 0.01], \n",
    "    'gamma': [0,0.25,0.5,1.0,1.5,2,5], \n",
    "    'max_depth': range(2, 10, 1), \n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0], \n",
    "    'reg_lambda': [0,1.0,10.0],\n",
    "    'scale_pos_weight': [1,3,5]}\n",
    "    \n",
    "xgb = XGBRegressor(\n",
    "    verbosity=1, \n",
    "    n_estimators=100)\n",
    "\n",
    "param_comb=10\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=2, \n",
    "    n_jobs=5, \n",
    "    cv=4, \n",
    "    random_state=42, \n",
    "    n_iter=param_comb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e403059b-1daf-4964-9ca4-db6c3065bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "Time elapsed: 705.7297909259796\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Time elapsed:', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b694460f-68c2-4152-bf80-37d1af9bf9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score\n",
      "-738.1786465368491\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 1.0, 'scale_pos_weight': 5, 'reg_lambda': 10.0, 'min_child_weight': 10, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 1.5, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "print('\\n Best score')\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('models/xgb-random-grid-search-results-cycle1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8de424e-7b51-4b0e-8500-fe9c79f28565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>172.096907</td>\n",
       "      <td>2.719855</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-705.824853</td>\n",
       "      <td>-751.704428</td>\n",
       "      <td>-750.970781</td>\n",
       "      <td>-744.214524</td>\n",
       "      <td>-738.178647</td>\n",
       "      <td>18.906251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.459052</td>\n",
       "      <td>6.082640</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-816.275026</td>\n",
       "      <td>-830.817184</td>\n",
       "      <td>-849.722174</td>\n",
       "      <td>-871.624163</td>\n",
       "      <td>-842.109637</td>\n",
       "      <td>20.760567</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.551664</td>\n",
       "      <td>4.674692</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-896.657547</td>\n",
       "      <td>-892.172079</td>\n",
       "      <td>-920.442263</td>\n",
       "      <td>-909.775774</td>\n",
       "      <td>-904.761916</td>\n",
       "      <td>11.126509</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.364851</td>\n",
       "      <td>6.897866</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-876.498722</td>\n",
       "      <td>-937.453677</td>\n",
       "      <td>-946.783762</td>\n",
       "      <td>-954.806455</td>\n",
       "      <td>-928.885654</td>\n",
       "      <td>30.862728</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67.023075</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>0.089501</td>\n",
       "      <td>0.019653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-1040.225263</td>\n",
       "      <td>-1086.958107</td>\n",
       "      <td>-1106.713273</td>\n",
       "      <td>-1113.458576</td>\n",
       "      <td>-1086.838805</td>\n",
       "      <td>28.620112</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.537812</td>\n",
       "      <td>0.860218</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-2081.355900</td>\n",
       "      <td>-2130.765947</td>\n",
       "      <td>-2140.441487</td>\n",
       "      <td>-2157.622215</td>\n",
       "      <td>-2127.546387</td>\n",
       "      <td>28.349452</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.349120</td>\n",
       "      <td>6.859317</td>\n",
       "      <td>0.087999</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-3878.364741</td>\n",
       "      <td>-3938.255726</td>\n",
       "      <td>-3903.741477</td>\n",
       "      <td>-3934.352049</td>\n",
       "      <td>-3913.678498</td>\n",
       "      <td>24.378483</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.398466</td>\n",
       "      <td>4.454418</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3924.902285</td>\n",
       "      <td>-3990.921698</td>\n",
       "      <td>-3952.124769</td>\n",
       "      <td>-3988.484724</td>\n",
       "      <td>-3964.108369</td>\n",
       "      <td>27.358202</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.090849</td>\n",
       "      <td>4.433393</td>\n",
       "      <td>0.095831</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3938.745855</td>\n",
       "      <td>-4020.001202</td>\n",
       "      <td>-3977.971223</td>\n",
       "      <td>-4033.261421</td>\n",
       "      <td>-3992.494925</td>\n",
       "      <td>37.142977</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.919646</td>\n",
       "      <td>0.360364</td>\n",
       "      <td>0.062261</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-4051.253910</td>\n",
       "      <td>-4105.651145</td>\n",
       "      <td>-4072.919965</td>\n",
       "      <td>-4115.946656</td>\n",
       "      <td>-4086.442919</td>\n",
       "      <td>25.790323</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6     172.096907      2.719855         0.070335        0.013558   \n",
       "4      59.459052      6.082640         0.081001        0.008687   \n",
       "1      46.551664      4.674692         0.086752        0.020264   \n",
       "9      90.364851      6.897866         0.055500        0.010805   \n",
       "8      67.023075      0.166826         0.089501        0.019653   \n",
       "5      24.537812      0.860218         0.064751        0.018940   \n",
       "3      87.349120      6.859317         0.087999        0.021989   \n",
       "7      88.398466      4.454418         0.090253        0.013609   \n",
       "2     105.090849      4.433393         0.095831        0.032114   \n",
       "0      48.919646      0.360364         0.062261        0.004450   \n",
       "\n",
       "   param_subsample  param_scale_pos_weight  param_reg_lambda  \\\n",
       "6              1.0                       5              10.0   \n",
       "4              0.6                       5               1.0   \n",
       "1              1.0                       3              10.0   \n",
       "9              0.6                       1              10.0   \n",
       "8              1.0                       3              10.0   \n",
       "5              0.8                       3               1.0   \n",
       "3              0.8                       1               0.0   \n",
       "7              0.6                       3               1.0   \n",
       "2              0.8                       3              10.0   \n",
       "0              0.6                       1               0.0   \n",
       "\n",
       "   param_min_child_weight  param_max_depth  param_learning_rate  param_gamma  \\\n",
       "6                      10                9                 0.10         1.50   \n",
       "4                       1                5                 0.10         1.50   \n",
       "1                       5                4                 0.30         0.00   \n",
       "9                      10                7                 0.05         5.00   \n",
       "8                       1                5                 0.05         2.00   \n",
       "5                      10                2                 0.05         5.00   \n",
       "3                       5                7                 0.01         0.25   \n",
       "7                      10                7                 0.01         1.50   \n",
       "2                       5                7                 0.01         0.50   \n",
       "0                       1                5                 0.01         1.50   \n",
       "\n",
       "   param_colsample_bytree                                             params  \\\n",
       "6                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "4                     0.8  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "1                     0.6  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "8                     0.8  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "5                     0.8  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "3                     0.6  {'subsample': 0.8, 'scale_pos_weight': 1, 'reg...   \n",
       "7                     0.6  {'subsample': 0.6, 'scale_pos_weight': 3, 'reg...   \n",
       "2                     1.0  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "0                     0.6  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "6        -705.824853        -751.704428        -750.970781        -744.214524   \n",
       "4        -816.275026        -830.817184        -849.722174        -871.624163   \n",
       "1        -896.657547        -892.172079        -920.442263        -909.775774   \n",
       "9        -876.498722        -937.453677        -946.783762        -954.806455   \n",
       "8       -1040.225263       -1086.958107       -1106.713273       -1113.458576   \n",
       "5       -2081.355900       -2130.765947       -2140.441487       -2157.622215   \n",
       "3       -3878.364741       -3938.255726       -3903.741477       -3934.352049   \n",
       "7       -3924.902285       -3990.921698       -3952.124769       -3988.484724   \n",
       "2       -3938.745855       -4020.001202       -3977.971223       -4033.261421   \n",
       "0       -4051.253910       -4105.651145       -4072.919965       -4115.946656   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "6      -738.178647       18.906251                1  \n",
       "4      -842.109637       20.760567                2  \n",
       "1      -904.761916       11.126509                3  \n",
       "9      -928.885654       30.862728                4  \n",
       "8     -1086.838805       28.620112                5  \n",
       "5     -2127.546387       28.349452                6  \n",
       "3     -3913.678498       24.378483                7  \n",
       "7     -3964.108369       27.358202                8  \n",
       "2     -3992.494925       37.142977                9  \n",
       "0     -4086.442919       25.790323               10  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.concat([pd.read_csv('models/xgb-random-grid-search-results-01.csv'), pd.read_csv('models/xgb-random-grid-search-results-02.csv')])\n",
    "df = pd.read_csv('models/xgb-random-grid-search-results-cycle1.csv')\n",
    "df.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9d1c1-4849-4497-868c-831328711a8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning cycle 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237be706-c44a-41cb-bb1f-f349b08bb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d7d72ab-ebac-4881-88c5-c7d709f41473",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.5, 0.3, 0.1, 0.05], \n",
    "    'gamma': [0,0.25,0.5,1.0,1.5,2,5], \n",
    "    'max_depth': range(2, 10, 1), \n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0], \n",
    "    'reg_lambda': [0,1.0,10.0],\n",
    "    'scale_pos_weight': [2,4,5]}\n",
    "    \n",
    "xgb = XGBRegressor(\n",
    "    verbosity=1, \n",
    "    n_estimators=100)\n",
    "\n",
    "param_comb=10\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=2, \n",
    "    n_jobs=5, \n",
    "    cv=4, \n",
    "    random_state=2, \n",
    "    n_iter=param_comb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92c84ba2-f053-4077-83f7-acea4efcd710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "Time elapsed: 681.6091592311859\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Time elapsed:', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d44b2d9-db95-4b5f-8fe0-6443c4f3412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score\n",
      "-739.3756526385001\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.8, 'scale_pos_weight': 4, 'reg_lambda': 1.0, 'min_child_weight': 10, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 1.5, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "print('\\n Best score')\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('models/xgb-random-grid-search-results-cycle2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cf369fa-4aae-4043-ac0e-1ab417036056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>172.096907</td>\n",
       "      <td>2.719855</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-705.824853</td>\n",
       "      <td>-751.704428</td>\n",
       "      <td>-750.970781</td>\n",
       "      <td>-744.214524</td>\n",
       "      <td>-738.178647</td>\n",
       "      <td>18.906251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95.661727</td>\n",
       "      <td>1.274809</td>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-708.849936</td>\n",
       "      <td>-755.094590</td>\n",
       "      <td>-746.628315</td>\n",
       "      <td>-746.929770</td>\n",
       "      <td>-739.375653</td>\n",
       "      <td>17.948330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.459052</td>\n",
       "      <td>6.082640</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-816.275026</td>\n",
       "      <td>-830.817184</td>\n",
       "      <td>-849.722174</td>\n",
       "      <td>-871.624163</td>\n",
       "      <td>-842.109637</td>\n",
       "      <td>20.760567</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.915283</td>\n",
       "      <td>6.262307</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-742.984840</td>\n",
       "      <td>-778.268668</td>\n",
       "      <td>-773.638945</td>\n",
       "      <td>-761.212157</td>\n",
       "      <td>-764.026152</td>\n",
       "      <td>13.655681</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.551664</td>\n",
       "      <td>4.674692</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-896.657547</td>\n",
       "      <td>-892.172079</td>\n",
       "      <td>-920.442263</td>\n",
       "      <td>-909.775774</td>\n",
       "      <td>-904.761916</td>\n",
       "      <td>11.126509</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97.990580</td>\n",
       "      <td>1.485266</td>\n",
       "      <td>0.074750</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-734.770261</td>\n",
       "      <td>-816.970395</td>\n",
       "      <td>-816.285486</td>\n",
       "      <td>-804.483983</td>\n",
       "      <td>-793.127531</td>\n",
       "      <td>34.056252</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.364851</td>\n",
       "      <td>6.897866</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-876.498722</td>\n",
       "      <td>-937.453677</td>\n",
       "      <td>-946.783762</td>\n",
       "      <td>-954.806455</td>\n",
       "      <td>-928.885654</td>\n",
       "      <td>30.862728</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>124.464072</td>\n",
       "      <td>8.989758</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.027433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-843.257513</td>\n",
       "      <td>-896.572650</td>\n",
       "      <td>-907.570839</td>\n",
       "      <td>-912.877997</td>\n",
       "      <td>-890.069750</td>\n",
       "      <td>27.659435</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67.023075</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>0.089501</td>\n",
       "      <td>0.019653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-1040.225263</td>\n",
       "      <td>-1086.958107</td>\n",
       "      <td>-1106.713273</td>\n",
       "      <td>-1113.458576</td>\n",
       "      <td>-1086.838805</td>\n",
       "      <td>28.620112</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.756507</td>\n",
       "      <td>0.786464</td>\n",
       "      <td>0.065499</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-952.215285</td>\n",
       "      <td>-926.965935</td>\n",
       "      <td>-953.281038</td>\n",
       "      <td>-950.545739</td>\n",
       "      <td>-945.751999</td>\n",
       "      <td>10.889865</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.903574</td>\n",
       "      <td>1.556440</td>\n",
       "      <td>0.059249</td>\n",
       "      <td>0.010426</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-1041.048260</td>\n",
       "      <td>-1013.927135</td>\n",
       "      <td>-1038.385400</td>\n",
       "      <td>-1010.158875</td>\n",
       "      <td>-1025.879917</td>\n",
       "      <td>13.932748</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.537812</td>\n",
       "      <td>0.860218</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-2081.355900</td>\n",
       "      <td>-2130.765947</td>\n",
       "      <td>-2140.441487</td>\n",
       "      <td>-2157.622215</td>\n",
       "      <td>-2127.546387</td>\n",
       "      <td>28.349452</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.349120</td>\n",
       "      <td>6.859317</td>\n",
       "      <td>0.087999</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-3878.364741</td>\n",
       "      <td>-3938.255726</td>\n",
       "      <td>-3903.741477</td>\n",
       "      <td>-3934.352049</td>\n",
       "      <td>-3913.678498</td>\n",
       "      <td>24.378483</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156.276887</td>\n",
       "      <td>1.930412</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1016.199290</td>\n",
       "      <td>-1065.083817</td>\n",
       "      <td>-1030.744470</td>\n",
       "      <td>-1070.140746</td>\n",
       "      <td>-1045.542081</td>\n",
       "      <td>22.731819</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34.653162</td>\n",
       "      <td>5.391651</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1018.952862</td>\n",
       "      <td>-1089.429055</td>\n",
       "      <td>-1013.041958</td>\n",
       "      <td>-1077.495338</td>\n",
       "      <td>-1049.729803</td>\n",
       "      <td>34.059410</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.398466</td>\n",
       "      <td>4.454418</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3924.902285</td>\n",
       "      <td>-3990.921698</td>\n",
       "      <td>-3952.124769</td>\n",
       "      <td>-3988.484724</td>\n",
       "      <td>-3964.108369</td>\n",
       "      <td>27.358202</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.983130</td>\n",
       "      <td>0.884607</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.022509</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1049.279684</td>\n",
       "      <td>-1089.646551</td>\n",
       "      <td>-1047.812713</td>\n",
       "      <td>-1078.236038</td>\n",
       "      <td>-1066.243747</td>\n",
       "      <td>18.158942</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.090849</td>\n",
       "      <td>4.433393</td>\n",
       "      <td>0.095831</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3938.745855</td>\n",
       "      <td>-4020.001202</td>\n",
       "      <td>-3977.971223</td>\n",
       "      <td>-4033.261421</td>\n",
       "      <td>-3992.494925</td>\n",
       "      <td>37.142977</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.369639</td>\n",
       "      <td>0.408713</td>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-1203.488754</td>\n",
       "      <td>-1251.913304</td>\n",
       "      <td>-1238.716929</td>\n",
       "      <td>-1260.772950</td>\n",
       "      <td>-1238.722984</td>\n",
       "      <td>21.803879</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.919646</td>\n",
       "      <td>0.360364</td>\n",
       "      <td>0.062261</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-4051.253910</td>\n",
       "      <td>-4105.651145</td>\n",
       "      <td>-4072.919965</td>\n",
       "      <td>-4115.946656</td>\n",
       "      <td>-4086.442919</td>\n",
       "      <td>25.790323</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6     172.096907      2.719855         0.070335        0.013558   \n",
       "3      95.661727      1.274809         0.082250        0.005118   \n",
       "4      59.459052      6.082640         0.081001        0.008687   \n",
       "1      79.915283      6.262307         0.080500        0.014291   \n",
       "1      46.551664      4.674692         0.086752        0.020264   \n",
       "8      97.990580      1.485266         0.074750        0.012872   \n",
       "9      90.364851      6.897866         0.055500        0.010805   \n",
       "5     124.464072      8.989758         0.093300        0.027433   \n",
       "8      67.023075      0.166826         0.089501        0.019653   \n",
       "0      92.756507      0.786464         0.065499        0.006537   \n",
       "7      43.903574      1.556440         0.059249        0.010426   \n",
       "5      24.537812      0.860218         0.064751        0.018940   \n",
       "3      87.349120      6.859317         0.087999        0.021989   \n",
       "4     156.276887      1.930412         0.088130        0.014824   \n",
       "9      34.653162      5.391651         0.059999        0.015083   \n",
       "7      88.398466      4.454418         0.090253        0.013609   \n",
       "6      34.983130      0.884607         0.066703        0.022509   \n",
       "2     105.090849      4.433393         0.095831        0.032114   \n",
       "2      28.369639      0.408713         0.073752        0.011713   \n",
       "0      48.919646      0.360364         0.062261        0.004450   \n",
       "\n",
       "   param_subsample  param_scale_pos_weight  param_reg_lambda  \\\n",
       "6              1.0                       5              10.0   \n",
       "3              0.8                       4               1.0   \n",
       "4              0.6                       5               1.0   \n",
       "1              0.6                       2               0.0   \n",
       "1              1.0                       3              10.0   \n",
       "8              1.0                       5               0.0   \n",
       "9              0.6                       1              10.0   \n",
       "5              1.0                       4              10.0   \n",
       "8              1.0                       3              10.0   \n",
       "0              1.0                       4               1.0   \n",
       "7              0.6                       2               1.0   \n",
       "5              0.8                       3               1.0   \n",
       "3              0.8                       1               0.0   \n",
       "4              0.8                       5               1.0   \n",
       "9              0.6                       5               1.0   \n",
       "7              0.6                       3               1.0   \n",
       "6              0.8                       5              10.0   \n",
       "2              0.8                       3              10.0   \n",
       "2              0.8                       4              10.0   \n",
       "0              0.6                       1               0.0   \n",
       "\n",
       "   param_min_child_weight  param_max_depth  param_learning_rate  param_gamma  \\\n",
       "6                      10                9                 0.10         1.50   \n",
       "3                      10                7                 0.10         1.50   \n",
       "4                       1                5                 0.10         1.50   \n",
       "1                       5                6                 0.10         5.00   \n",
       "1                       5                4                 0.30         0.00   \n",
       "8                       5                7                 0.10         2.00   \n",
       "9                      10                7                 0.05         5.00   \n",
       "5                      10                8                 0.05         1.50   \n",
       "8                       1                5                 0.05         2.00   \n",
       "0                      10                6                 0.50         0.50   \n",
       "7                       1                4                 0.50         5.00   \n",
       "5                      10                2                 0.05         5.00   \n",
       "3                       5                7                 0.01         0.25   \n",
       "4                       1                8                 0.50         1.00   \n",
       "9                      10                4                 0.50         1.00   \n",
       "7                      10                7                 0.01         1.50   \n",
       "6                       1                3                 0.30         0.00   \n",
       "2                       5                7                 0.01         0.50   \n",
       "2                      10                3                 0.10         0.50   \n",
       "0                       1                5                 0.01         1.50   \n",
       "\n",
       "   param_colsample_bytree                                             params  \\\n",
       "6                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "3                     0.8  {'subsample': 0.8, 'scale_pos_weight': 4, 'reg...   \n",
       "4                     0.8  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "1                     0.8  {'subsample': 0.6, 'scale_pos_weight': 2, 'reg...   \n",
       "1                     0.6  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "8                     0.6  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "5                     0.8  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "8                     0.8  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "0                     0.8  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "7                     0.8  {'subsample': 0.6, 'scale_pos_weight': 2, 'reg...   \n",
       "5                     0.8  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "3                     0.6  {'subsample': 0.8, 'scale_pos_weight': 1, 'reg...   \n",
       "4                     1.0  {'subsample': 0.8, 'scale_pos_weight': 5, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "7                     0.6  {'subsample': 0.6, 'scale_pos_weight': 3, 'reg...   \n",
       "6                     0.8  {'subsample': 0.8, 'scale_pos_weight': 5, 'reg...   \n",
       "2                     1.0  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "2                     0.6  {'subsample': 0.8, 'scale_pos_weight': 4, 'reg...   \n",
       "0                     0.6  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "6        -705.824853        -751.704428        -750.970781        -744.214524   \n",
       "3        -708.849936        -755.094590        -746.628315        -746.929770   \n",
       "4        -816.275026        -830.817184        -849.722174        -871.624163   \n",
       "1        -742.984840        -778.268668        -773.638945        -761.212157   \n",
       "1        -896.657547        -892.172079        -920.442263        -909.775774   \n",
       "8        -734.770261        -816.970395        -816.285486        -804.483983   \n",
       "9        -876.498722        -937.453677        -946.783762        -954.806455   \n",
       "5        -843.257513        -896.572650        -907.570839        -912.877997   \n",
       "8       -1040.225263       -1086.958107       -1106.713273       -1113.458576   \n",
       "0        -952.215285        -926.965935        -953.281038        -950.545739   \n",
       "7       -1041.048260       -1013.927135       -1038.385400       -1010.158875   \n",
       "5       -2081.355900       -2130.765947       -2140.441487       -2157.622215   \n",
       "3       -3878.364741       -3938.255726       -3903.741477       -3934.352049   \n",
       "4       -1016.199290       -1065.083817       -1030.744470       -1070.140746   \n",
       "9       -1018.952862       -1089.429055       -1013.041958       -1077.495338   \n",
       "7       -3924.902285       -3990.921698       -3952.124769       -3988.484724   \n",
       "6       -1049.279684       -1089.646551       -1047.812713       -1078.236038   \n",
       "2       -3938.745855       -4020.001202       -3977.971223       -4033.261421   \n",
       "2       -1203.488754       -1251.913304       -1238.716929       -1260.772950   \n",
       "0       -4051.253910       -4105.651145       -4072.919965       -4115.946656   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "6      -738.178647       18.906251                1  \n",
       "3      -739.375653       17.948330                1  \n",
       "4      -842.109637       20.760567                2  \n",
       "1      -764.026152       13.655681                2  \n",
       "1      -904.761916       11.126509                3  \n",
       "8      -793.127531       34.056252                3  \n",
       "9      -928.885654       30.862728                4  \n",
       "5      -890.069750       27.659435                4  \n",
       "8     -1086.838805       28.620112                5  \n",
       "0      -945.751999       10.889865                5  \n",
       "7     -1025.879917       13.932748                6  \n",
       "5     -2127.546387       28.349452                6  \n",
       "3     -3913.678498       24.378483                7  \n",
       "4     -1045.542081       22.731819                7  \n",
       "9     -1049.729803       34.059410                8  \n",
       "7     -3964.108369       27.358202                8  \n",
       "6     -1066.243747       18.158942                9  \n",
       "2     -3992.494925       37.142977                9  \n",
       "2     -1238.722984       21.803879               10  \n",
       "0     -4086.442919       25.790323               10  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv('models/xgb-random-grid-search-results-cycle1.csv'), pd.read_csv('models/xgb-random-grid-search-results-cycle2.csv')])\n",
    "df.sort_values('rank_test_score')#.groupby('param_max_depth').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a4528-b087-46ff-af8b-bde59c45b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "xgb = XGBRegressor(\n",
    "    verbosity=1, \n",
    "    n_estimators=150, \n",
    "    subsample=1.0, \n",
    "    scale_pos_weight=5, \n",
    "    reg_lambda=10.0, \n",
    "    min_child_weight=10, \n",
    "    max_depth=9, \n",
    "    learning_rate=0.1, \n",
    "    gamma=1.5, \n",
    "    colsample_bytree=1.0,\n",
    "    n_jobs=5)\n",
    "xgb.fit(X_train, y_train)\n",
    "with open('models/ensemble_xg_boosting_cycle2.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e441885-3671-4572-a511-392f0fa2b22d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning cycle 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3b66d37-394d-48bc-9586-f1f25cad47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bccb5c86-7343-4508-893a-b889fe6e6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.3, 0.1, 0.05], \n",
    "    'gamma': [0.5,1.0,1.5,2,5], \n",
    "    'max_depth': range(4, 10, 1), \n",
    "    'min_child_weight': [5, 7, 10],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0], \n",
    "    'reg_lambda': [0,1.0,10.0],\n",
    "    'scale_pos_weight': [2,4,5]}\n",
    "    \n",
    "xgb = XGBRegressor(\n",
    "    verbosity=1, \n",
    "    n_estimators=100)\n",
    "\n",
    "param_comb=10\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=2, \n",
    "    n_jobs=5, \n",
    "    cv=4, \n",
    "    random_state=3, \n",
    "    n_iter=param_comb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfbb5c29-0b1b-4315-90aa-44f4513aaf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "Time elapsed: 756.3472821712494\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Time elapsed:', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b498689a-ddee-4d5c-aadc-a44c89b96b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score\n",
      "-759.705011283515\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.7, 'scale_pos_weight': 2, 'reg_lambda': 10.0, 'min_child_weight': 5, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 1.5, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "print('\\n Best score')\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('models/xgb-random-grid-search-results-cycle3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c35937fa-ac5f-4e81-84bd-78b9520d19e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.919646</td>\n",
       "      <td>0.360364</td>\n",
       "      <td>0.062261</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-4051.253910</td>\n",
       "      <td>-4105.651145</td>\n",
       "      <td>-4072.919965</td>\n",
       "      <td>-4115.946656</td>\n",
       "      <td>-4086.442919</td>\n",
       "      <td>25.790323</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.090849</td>\n",
       "      <td>4.433393</td>\n",
       "      <td>0.095831</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3938.745855</td>\n",
       "      <td>-4020.001202</td>\n",
       "      <td>-3977.971223</td>\n",
       "      <td>-4033.261421</td>\n",
       "      <td>-3992.494925</td>\n",
       "      <td>37.142977</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.398466</td>\n",
       "      <td>4.454418</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3924.902285</td>\n",
       "      <td>-3990.921698</td>\n",
       "      <td>-3952.124769</td>\n",
       "      <td>-3988.484724</td>\n",
       "      <td>-3964.108369</td>\n",
       "      <td>27.358202</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.349120</td>\n",
       "      <td>6.859317</td>\n",
       "      <td>0.087999</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-3878.364741</td>\n",
       "      <td>-3938.255726</td>\n",
       "      <td>-3903.741477</td>\n",
       "      <td>-3934.352049</td>\n",
       "      <td>-3913.678498</td>\n",
       "      <td>24.378483</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.537812</td>\n",
       "      <td>0.860218</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-2081.355900</td>\n",
       "      <td>-2130.765947</td>\n",
       "      <td>-2140.441487</td>\n",
       "      <td>-2157.622215</td>\n",
       "      <td>-2127.546387</td>\n",
       "      <td>28.349452</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.369639</td>\n",
       "      <td>0.408713</td>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-1203.488754</td>\n",
       "      <td>-1251.913304</td>\n",
       "      <td>-1238.716929</td>\n",
       "      <td>-1260.772950</td>\n",
       "      <td>-1238.722984</td>\n",
       "      <td>21.803879</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67.023075</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>0.089501</td>\n",
       "      <td>0.019653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-1040.225263</td>\n",
       "      <td>-1086.958107</td>\n",
       "      <td>-1106.713273</td>\n",
       "      <td>-1113.458576</td>\n",
       "      <td>-1086.838805</td>\n",
       "      <td>28.620112</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.983130</td>\n",
       "      <td>0.884607</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.022509</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1049.279684</td>\n",
       "      <td>-1089.646551</td>\n",
       "      <td>-1047.812713</td>\n",
       "      <td>-1078.236038</td>\n",
       "      <td>-1066.243747</td>\n",
       "      <td>18.158942</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34.653162</td>\n",
       "      <td>5.391651</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1018.952862</td>\n",
       "      <td>-1089.429055</td>\n",
       "      <td>-1013.041958</td>\n",
       "      <td>-1077.495338</td>\n",
       "      <td>-1049.729803</td>\n",
       "      <td>34.059410</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156.276887</td>\n",
       "      <td>1.930412</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1016.199290</td>\n",
       "      <td>-1065.083817</td>\n",
       "      <td>-1030.744470</td>\n",
       "      <td>-1070.140746</td>\n",
       "      <td>-1045.542081</td>\n",
       "      <td>22.731819</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.687946</td>\n",
       "      <td>8.045095</td>\n",
       "      <td>0.081751</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-999.903012</td>\n",
       "      <td>-1035.112921</td>\n",
       "      <td>-1054.307465</td>\n",
       "      <td>-1055.026615</td>\n",
       "      <td>-1036.087503</td>\n",
       "      <td>22.365845</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.903574</td>\n",
       "      <td>1.556440</td>\n",
       "      <td>0.059249</td>\n",
       "      <td>0.010426</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-1041.048260</td>\n",
       "      <td>-1013.927135</td>\n",
       "      <td>-1038.385400</td>\n",
       "      <td>-1010.158875</td>\n",
       "      <td>-1025.879917</td>\n",
       "      <td>13.932748</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.756507</td>\n",
       "      <td>0.786464</td>\n",
       "      <td>0.065499</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-952.215285</td>\n",
       "      <td>-926.965935</td>\n",
       "      <td>-953.281038</td>\n",
       "      <td>-950.545739</td>\n",
       "      <td>-945.751999</td>\n",
       "      <td>10.889865</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.364851</td>\n",
       "      <td>6.897866</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-876.498722</td>\n",
       "      <td>-937.453677</td>\n",
       "      <td>-946.783762</td>\n",
       "      <td>-954.806455</td>\n",
       "      <td>-928.885654</td>\n",
       "      <td>30.862728</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.036589</td>\n",
       "      <td>0.550902</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-885.398644</td>\n",
       "      <td>-930.132292</td>\n",
       "      <td>-920.900980</td>\n",
       "      <td>-920.817962</td>\n",
       "      <td>-914.312470</td>\n",
       "      <td>17.117287</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72.822142</td>\n",
       "      <td>12.738063</td>\n",
       "      <td>0.069749</td>\n",
       "      <td>0.022765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-861.133416</td>\n",
       "      <td>-916.009535</td>\n",
       "      <td>-927.876145</td>\n",
       "      <td>-928.927074</td>\n",
       "      <td>-908.486543</td>\n",
       "      <td>27.805962</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.551664</td>\n",
       "      <td>4.674692</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-896.657547</td>\n",
       "      <td>-892.172079</td>\n",
       "      <td>-920.442263</td>\n",
       "      <td>-909.775774</td>\n",
       "      <td>-904.761916</td>\n",
       "      <td>11.126509</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>124.464072</td>\n",
       "      <td>8.989758</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.027433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-843.257513</td>\n",
       "      <td>-896.572650</td>\n",
       "      <td>-907.570839</td>\n",
       "      <td>-912.877997</td>\n",
       "      <td>-890.069750</td>\n",
       "      <td>27.659435</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81.068176</td>\n",
       "      <td>8.011820</td>\n",
       "      <td>0.066516</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-848.854772</td>\n",
       "      <td>-849.573507</td>\n",
       "      <td>-848.826881</td>\n",
       "      <td>-888.621607</td>\n",
       "      <td>-858.969192</td>\n",
       "      <td>17.122446</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70.189498</td>\n",
       "      <td>7.894462</td>\n",
       "      <td>0.053494</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-823.144510</td>\n",
       "      <td>-859.516029</td>\n",
       "      <td>-872.363278</td>\n",
       "      <td>-880.776403</td>\n",
       "      <td>-858.950055</td>\n",
       "      <td>22.015116</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.092149</td>\n",
       "      <td>0.703048</td>\n",
       "      <td>1.561830</td>\n",
       "      <td>1.566615</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-819.904949</td>\n",
       "      <td>-866.805805</td>\n",
       "      <td>-869.372431</td>\n",
       "      <td>-857.945057</td>\n",
       "      <td>-853.507060</td>\n",
       "      <td>19.858023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.310222</td>\n",
       "      <td>3.984922</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-818.324497</td>\n",
       "      <td>-836.542984</td>\n",
       "      <td>-864.249680</td>\n",
       "      <td>-857.328862</td>\n",
       "      <td>-844.111506</td>\n",
       "      <td>18.045061</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.459052</td>\n",
       "      <td>6.082640</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-816.275026</td>\n",
       "      <td>-830.817184</td>\n",
       "      <td>-849.722174</td>\n",
       "      <td>-871.624163</td>\n",
       "      <td>-842.109637</td>\n",
       "      <td>20.760567</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140.812081</td>\n",
       "      <td>3.382543</td>\n",
       "      <td>0.092250</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-783.711073</td>\n",
       "      <td>-857.406269</td>\n",
       "      <td>-860.130546</td>\n",
       "      <td>-855.781006</td>\n",
       "      <td>-839.257223</td>\n",
       "      <td>32.107218</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97.990580</td>\n",
       "      <td>1.485266</td>\n",
       "      <td>0.074750</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-734.770261</td>\n",
       "      <td>-816.970395</td>\n",
       "      <td>-816.285486</td>\n",
       "      <td>-804.483983</td>\n",
       "      <td>-793.127531</td>\n",
       "      <td>34.056252</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.414182</td>\n",
       "      <td>2.774547</td>\n",
       "      <td>0.070379</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-734.918585</td>\n",
       "      <td>-774.892424</td>\n",
       "      <td>-771.022868</td>\n",
       "      <td>-780.773757</td>\n",
       "      <td>-765.401909</td>\n",
       "      <td>17.938728</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.915283</td>\n",
       "      <td>6.262307</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-742.984840</td>\n",
       "      <td>-778.268668</td>\n",
       "      <td>-773.638945</td>\n",
       "      <td>-761.212157</td>\n",
       "      <td>-764.026152</td>\n",
       "      <td>13.655681</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153.151929</td>\n",
       "      <td>2.407120</td>\n",
       "      <td>0.095937</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-713.970589</td>\n",
       "      <td>-769.567490</td>\n",
       "      <td>-788.304253</td>\n",
       "      <td>-766.977714</td>\n",
       "      <td>-759.705011</td>\n",
       "      <td>27.657344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95.661727</td>\n",
       "      <td>1.274809</td>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-708.849936</td>\n",
       "      <td>-755.094590</td>\n",
       "      <td>-746.628315</td>\n",
       "      <td>-746.929770</td>\n",
       "      <td>-739.375653</td>\n",
       "      <td>17.948330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>172.096907</td>\n",
       "      <td>2.719855</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-705.824853</td>\n",
       "      <td>-751.704428</td>\n",
       "      <td>-750.970781</td>\n",
       "      <td>-744.214524</td>\n",
       "      <td>-738.178647</td>\n",
       "      <td>18.906251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      48.919646      0.360364         0.062261        0.004450   \n",
       "2     105.090849      4.433393         0.095831        0.032114   \n",
       "7      88.398466      4.454418         0.090253        0.013609   \n",
       "3      87.349120      6.859317         0.087999        0.021989   \n",
       "5      24.537812      0.860218         0.064751        0.018940   \n",
       "2      28.369639      0.408713         0.073752        0.011713   \n",
       "8      67.023075      0.166826         0.089501        0.019653   \n",
       "6      34.983130      0.884607         0.066703        0.022509   \n",
       "9      34.653162      5.391651         0.059999        0.015083   \n",
       "4     156.276887      1.930412         0.088130        0.014824   \n",
       "5      76.687946      8.045095         0.081751        0.011755   \n",
       "7      43.903574      1.556440         0.059249        0.010426   \n",
       "0      92.756507      0.786464         0.065499        0.006537   \n",
       "9      90.364851      6.897866         0.055500        0.010805   \n",
       "0      62.036589      0.550902         0.090500        0.014566   \n",
       "9      72.822142     12.738063         0.069749        0.022765   \n",
       "1      46.551664      4.674692         0.086752        0.020264   \n",
       "5     124.464072      8.989758         0.093300        0.027433   \n",
       "6      81.068176      8.011820         0.066516        0.008543   \n",
       "8      70.189498      7.894462         0.053494        0.009343   \n",
       "4      56.092149      0.703048         1.561830        1.566615   \n",
       "1      71.310222      3.984922         0.078000        0.009356   \n",
       "4      59.459052      6.082640         0.081001        0.008687   \n",
       "2     140.812081      3.382543         0.092250        0.009120   \n",
       "8      97.990580      1.485266         0.074750        0.012872   \n",
       "3      83.414182      2.774547         0.070379        0.005891   \n",
       "1      79.915283      6.262307         0.080500        0.014291   \n",
       "7     153.151929      2.407120         0.095937        0.011148   \n",
       "3      95.661727      1.274809         0.082250        0.005118   \n",
       "6     172.096907      2.719855         0.070335        0.013558   \n",
       "\n",
       "   param_subsample  param_scale_pos_weight  param_reg_lambda  \\\n",
       "0              0.6                       1               0.0   \n",
       "2              0.8                       3              10.0   \n",
       "7              0.6                       3               1.0   \n",
       "3              0.8                       1               0.0   \n",
       "5              0.8                       3               1.0   \n",
       "2              0.8                       4              10.0   \n",
       "8              1.0                       3              10.0   \n",
       "6              0.8                       5              10.0   \n",
       "9              0.6                       5               1.0   \n",
       "4              0.8                       5               1.0   \n",
       "5              1.0                       2               0.0   \n",
       "7              0.6                       2               1.0   \n",
       "0              1.0                       4               1.0   \n",
       "9              0.6                       1              10.0   \n",
       "0              0.7                       2               0.0   \n",
       "9              1.0                       4               1.0   \n",
       "1              1.0                       3              10.0   \n",
       "5              1.0                       4              10.0   \n",
       "6              1.0                       2               0.0   \n",
       "8              1.0                       5              10.0   \n",
       "4              0.8                       2               0.0   \n",
       "1              1.0                       4               1.0   \n",
       "4              0.6                       5               1.0   \n",
       "2              1.0                       2               1.0   \n",
       "8              1.0                       5               0.0   \n",
       "3              0.7                       5               0.0   \n",
       "1              0.6                       2               0.0   \n",
       "7              0.7                       2              10.0   \n",
       "3              0.8                       4               1.0   \n",
       "6              1.0                       5              10.0   \n",
       "\n",
       "   param_min_child_weight  param_max_depth  param_learning_rate  param_gamma  \\\n",
       "0                       1                5                 0.01         1.50   \n",
       "2                       5                7                 0.01         0.50   \n",
       "7                      10                7                 0.01         1.50   \n",
       "3                       5                7                 0.01         0.25   \n",
       "5                      10                2                 0.05         5.00   \n",
       "2                      10                3                 0.10         0.50   \n",
       "8                       1                5                 0.05         2.00   \n",
       "6                       1                3                 0.30         0.00   \n",
       "9                      10                4                 0.50         1.00   \n",
       "4                       1                8                 0.50         1.00   \n",
       "5                       5                5                 0.05         1.50   \n",
       "7                       1                4                 0.50         5.00   \n",
       "0                      10                6                 0.50         0.50   \n",
       "9                      10                7                 0.05         5.00   \n",
       "0                       5                6                 0.30         5.00   \n",
       "9                       5                7                 0.05         5.00   \n",
       "1                       5                4                 0.30         0.00   \n",
       "5                      10                8                 0.05         1.50   \n",
       "6                      10                5                 0.30         2.00   \n",
       "8                       5                5                 0.10         1.50   \n",
       "4                       5                5                 0.10         5.00   \n",
       "1                      10                6                 0.30         1.00   \n",
       "4                       1                5                 0.10         1.50   \n",
       "2                       5                8                 0.05         1.50   \n",
       "8                       5                7                 0.10         2.00   \n",
       "3                      10                6                 0.10         0.50   \n",
       "1                       5                6                 0.10         5.00   \n",
       "7                       5                9                 0.10         1.50   \n",
       "3                      10                7                 0.10         1.50   \n",
       "6                      10                9                 0.10         1.50   \n",
       "\n",
       "   param_colsample_bytree                                             params  \\\n",
       "0                     0.6  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "2                     1.0  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "7                     0.6  {'subsample': 0.6, 'scale_pos_weight': 3, 'reg...   \n",
       "3                     0.6  {'subsample': 0.8, 'scale_pos_weight': 1, 'reg...   \n",
       "5                     0.8  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "2                     0.6  {'subsample': 0.8, 'scale_pos_weight': 4, 'reg...   \n",
       "8                     0.8  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "6                     0.8  {'subsample': 0.8, 'scale_pos_weight': 5, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "4                     1.0  {'subsample': 0.8, 'scale_pos_weight': 5, 'reg...   \n",
       "5                     0.8  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "7                     0.8  {'subsample': 0.6, 'scale_pos_weight': 2, 'reg...   \n",
       "0                     0.8  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "0                     0.6  {'subsample': 0.7, 'scale_pos_weight': 2, 'reg...   \n",
       "9                     0.6  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "1                     0.6  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "5                     0.8  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "6                     1.0  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "8                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "4                     0.6  {'subsample': 0.8, 'scale_pos_weight': 2, 'reg...   \n",
       "1                     0.6  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "4                     0.8  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "2                     0.8  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "8                     0.6  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "3                     1.0  {'subsample': 0.7, 'scale_pos_weight': 5, 'reg...   \n",
       "1                     0.8  {'subsample': 0.6, 'scale_pos_weight': 2, 'reg...   \n",
       "7                     0.8  {'subsample': 0.7, 'scale_pos_weight': 2, 'reg...   \n",
       "3                     0.8  {'subsample': 0.8, 'scale_pos_weight': 4, 'reg...   \n",
       "6                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0       -4051.253910       -4105.651145       -4072.919965       -4115.946656   \n",
       "2       -3938.745855       -4020.001202       -3977.971223       -4033.261421   \n",
       "7       -3924.902285       -3990.921698       -3952.124769       -3988.484724   \n",
       "3       -3878.364741       -3938.255726       -3903.741477       -3934.352049   \n",
       "5       -2081.355900       -2130.765947       -2140.441487       -2157.622215   \n",
       "2       -1203.488754       -1251.913304       -1238.716929       -1260.772950   \n",
       "8       -1040.225263       -1086.958107       -1106.713273       -1113.458576   \n",
       "6       -1049.279684       -1089.646551       -1047.812713       -1078.236038   \n",
       "9       -1018.952862       -1089.429055       -1013.041958       -1077.495338   \n",
       "4       -1016.199290       -1065.083817       -1030.744470       -1070.140746   \n",
       "5        -999.903012       -1035.112921       -1054.307465       -1055.026615   \n",
       "7       -1041.048260       -1013.927135       -1038.385400       -1010.158875   \n",
       "0        -952.215285        -926.965935        -953.281038        -950.545739   \n",
       "9        -876.498722        -937.453677        -946.783762        -954.806455   \n",
       "0        -885.398644        -930.132292        -920.900980        -920.817962   \n",
       "9        -861.133416        -916.009535        -927.876145        -928.927074   \n",
       "1        -896.657547        -892.172079        -920.442263        -909.775774   \n",
       "5        -843.257513        -896.572650        -907.570839        -912.877997   \n",
       "6        -848.854772        -849.573507        -848.826881        -888.621607   \n",
       "8        -823.144510        -859.516029        -872.363278        -880.776403   \n",
       "4        -819.904949        -866.805805        -869.372431        -857.945057   \n",
       "1        -818.324497        -836.542984        -864.249680        -857.328862   \n",
       "4        -816.275026        -830.817184        -849.722174        -871.624163   \n",
       "2        -783.711073        -857.406269        -860.130546        -855.781006   \n",
       "8        -734.770261        -816.970395        -816.285486        -804.483983   \n",
       "3        -734.918585        -774.892424        -771.022868        -780.773757   \n",
       "1        -742.984840        -778.268668        -773.638945        -761.212157   \n",
       "7        -713.970589        -769.567490        -788.304253        -766.977714   \n",
       "3        -708.849936        -755.094590        -746.628315        -746.929770   \n",
       "6        -705.824853        -751.704428        -750.970781        -744.214524   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0     -4086.442919       25.790323               10  \n",
       "2     -3992.494925       37.142977                9  \n",
       "7     -3964.108369       27.358202                8  \n",
       "3     -3913.678498       24.378483                7  \n",
       "5     -2127.546387       28.349452                6  \n",
       "2     -1238.722984       21.803879               10  \n",
       "8     -1086.838805       28.620112                5  \n",
       "6     -1066.243747       18.158942                9  \n",
       "9     -1049.729803       34.059410                8  \n",
       "4     -1045.542081       22.731819                7  \n",
       "5     -1036.087503       22.365845               10  \n",
       "7     -1025.879917       13.932748                6  \n",
       "0      -945.751999       10.889865                5  \n",
       "9      -928.885654       30.862728                4  \n",
       "0      -914.312470       17.117287                9  \n",
       "9      -908.486543       27.805962                8  \n",
       "1      -904.761916       11.126509                3  \n",
       "5      -890.069750       27.659435                4  \n",
       "6      -858.969192       17.122446                7  \n",
       "8      -858.950055       22.015116                6  \n",
       "4      -853.507060       19.858023                5  \n",
       "1      -844.111506       18.045061                4  \n",
       "4      -842.109637       20.760567                2  \n",
       "2      -839.257223       32.107218                3  \n",
       "8      -793.127531       34.056252                3  \n",
       "3      -765.401909       17.938728                2  \n",
       "1      -764.026152       13.655681                2  \n",
       "7      -759.705011       27.657344                1  \n",
       "3      -739.375653       17.948330                1  \n",
       "6      -738.178647       18.906251                1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv('models/xgb-random-grid-search-results-cycle1.csv'), pd.read_csv('models/xgb-random-grid-search-results-cycle2.csv'), pd.read_csv('models/xgb-random-grid-search-results-cycle3.csv')])\n",
    "df.sort_values('mean_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f370c91-d58d-4fd0-95ad-c66194b64c53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning cycle 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349b3eb-3e37-4e7e-a2b5-3ca3a13ec7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f854ae1-3043-4732-ad6b-2e2169df7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.12, 0.1, 0.08], \n",
    "    'gamma': [0.5,1.0,1.5,2,5], \n",
    "    'max_depth': range(6, 10, 1), \n",
    "    'min_child_weight': [5, 8, 10],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0], \n",
    "    'reg_lambda': [0,1.0,10.0],\n",
    "    'scale_pos_weight': [2,4,5]}\n",
    "    \n",
    "xgb = XGBRegressor(\n",
    "    verbosity=1, \n",
    "    n_estimators=100)\n",
    "\n",
    "param_comb=10\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=2, \n",
    "    n_jobs=5, \n",
    "    cv=4, \n",
    "    random_state=4, \n",
    "    n_iter=param_comb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03631b57-4277-4365-9d24-a3be473b325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "Time elapsed: 1047.2771711349487\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Time elapsed:', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "738ccff3-2d71-42b8-aadb-8b71f0685f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score\n",
      "-738.7450668038117\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.9, 'scale_pos_weight': 5, 'reg_lambda': 1.0, 'min_child_weight': 10, 'max_depth': 9, 'learning_rate': 0.12, 'gamma': 1.5, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "print('\\n Best score')\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('models/xgb-random-grid-search-results-cycle4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d4a3b41-974d-43ee-82b3-40ae5ea09160",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.919646</td>\n",
       "      <td>0.360364</td>\n",
       "      <td>0.062261</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-4051.253910</td>\n",
       "      <td>-4105.651145</td>\n",
       "      <td>-4072.919965</td>\n",
       "      <td>-4115.946656</td>\n",
       "      <td>-4086.442919</td>\n",
       "      <td>25.790323</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.090849</td>\n",
       "      <td>4.433393</td>\n",
       "      <td>0.095831</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3938.745855</td>\n",
       "      <td>-4020.001202</td>\n",
       "      <td>-3977.971223</td>\n",
       "      <td>-4033.261421</td>\n",
       "      <td>-3992.494925</td>\n",
       "      <td>37.142977</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.398466</td>\n",
       "      <td>4.454418</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-3924.902285</td>\n",
       "      <td>-3990.921698</td>\n",
       "      <td>-3952.124769</td>\n",
       "      <td>-3988.484724</td>\n",
       "      <td>-3964.108369</td>\n",
       "      <td>27.358202</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.349120</td>\n",
       "      <td>6.859317</td>\n",
       "      <td>0.087999</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-3878.364741</td>\n",
       "      <td>-3938.255726</td>\n",
       "      <td>-3903.741477</td>\n",
       "      <td>-3934.352049</td>\n",
       "      <td>-3913.678498</td>\n",
       "      <td>24.378483</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.537812</td>\n",
       "      <td>0.860218</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-2081.355900</td>\n",
       "      <td>-2130.765947</td>\n",
       "      <td>-2140.441487</td>\n",
       "      <td>-2157.622215</td>\n",
       "      <td>-2127.546387</td>\n",
       "      <td>28.349452</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.369639</td>\n",
       "      <td>0.408713</td>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-1203.488754</td>\n",
       "      <td>-1251.913304</td>\n",
       "      <td>-1238.716929</td>\n",
       "      <td>-1260.772950</td>\n",
       "      <td>-1238.722984</td>\n",
       "      <td>21.803879</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67.023075</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>0.089501</td>\n",
       "      <td>0.019653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-1040.225263</td>\n",
       "      <td>-1086.958107</td>\n",
       "      <td>-1106.713273</td>\n",
       "      <td>-1113.458576</td>\n",
       "      <td>-1086.838805</td>\n",
       "      <td>28.620112</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.983130</td>\n",
       "      <td>0.884607</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.022509</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1049.279684</td>\n",
       "      <td>-1089.646551</td>\n",
       "      <td>-1047.812713</td>\n",
       "      <td>-1078.236038</td>\n",
       "      <td>-1066.243747</td>\n",
       "      <td>18.158942</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34.653162</td>\n",
       "      <td>5.391651</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1018.952862</td>\n",
       "      <td>-1089.429055</td>\n",
       "      <td>-1013.041958</td>\n",
       "      <td>-1077.495338</td>\n",
       "      <td>-1049.729803</td>\n",
       "      <td>34.059410</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156.276887</td>\n",
       "      <td>1.930412</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-1016.199290</td>\n",
       "      <td>-1065.083817</td>\n",
       "      <td>-1030.744470</td>\n",
       "      <td>-1070.140746</td>\n",
       "      <td>-1045.542081</td>\n",
       "      <td>22.731819</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.687946</td>\n",
       "      <td>8.045095</td>\n",
       "      <td>0.081751</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-999.903012</td>\n",
       "      <td>-1035.112921</td>\n",
       "      <td>-1054.307465</td>\n",
       "      <td>-1055.026615</td>\n",
       "      <td>-1036.087503</td>\n",
       "      <td>22.365845</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.903574</td>\n",
       "      <td>1.556440</td>\n",
       "      <td>0.059249</td>\n",
       "      <td>0.010426</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-1041.048260</td>\n",
       "      <td>-1013.927135</td>\n",
       "      <td>-1038.385400</td>\n",
       "      <td>-1010.158875</td>\n",
       "      <td>-1025.879917</td>\n",
       "      <td>13.932748</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.756507</td>\n",
       "      <td>0.786464</td>\n",
       "      <td>0.065499</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-952.215285</td>\n",
       "      <td>-926.965935</td>\n",
       "      <td>-953.281038</td>\n",
       "      <td>-950.545739</td>\n",
       "      <td>-945.751999</td>\n",
       "      <td>10.889865</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.364851</td>\n",
       "      <td>6.897866</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 1, 'reg...</td>\n",
       "      <td>-876.498722</td>\n",
       "      <td>-937.453677</td>\n",
       "      <td>-946.783762</td>\n",
       "      <td>-954.806455</td>\n",
       "      <td>-928.885654</td>\n",
       "      <td>30.862728</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.036589</td>\n",
       "      <td>0.550902</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-885.398644</td>\n",
       "      <td>-930.132292</td>\n",
       "      <td>-920.900980</td>\n",
       "      <td>-920.817962</td>\n",
       "      <td>-914.312470</td>\n",
       "      <td>17.117287</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72.822142</td>\n",
       "      <td>12.738063</td>\n",
       "      <td>0.069749</td>\n",
       "      <td>0.022765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-861.133416</td>\n",
       "      <td>-916.009535</td>\n",
       "      <td>-927.876145</td>\n",
       "      <td>-928.927074</td>\n",
       "      <td>-908.486543</td>\n",
       "      <td>27.805962</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.551664</td>\n",
       "      <td>4.674692</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 3, 'reg...</td>\n",
       "      <td>-896.657547</td>\n",
       "      <td>-892.172079</td>\n",
       "      <td>-920.442263</td>\n",
       "      <td>-909.775774</td>\n",
       "      <td>-904.761916</td>\n",
       "      <td>11.126509</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>124.464072</td>\n",
       "      <td>8.989758</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.027433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-843.257513</td>\n",
       "      <td>-896.572650</td>\n",
       "      <td>-907.570839</td>\n",
       "      <td>-912.877997</td>\n",
       "      <td>-890.069750</td>\n",
       "      <td>27.659435</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81.068176</td>\n",
       "      <td>8.011820</td>\n",
       "      <td>0.066516</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-848.854772</td>\n",
       "      <td>-849.573507</td>\n",
       "      <td>-848.826881</td>\n",
       "      <td>-888.621607</td>\n",
       "      <td>-858.969192</td>\n",
       "      <td>17.122446</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70.189498</td>\n",
       "      <td>7.894462</td>\n",
       "      <td>0.053494</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-823.144510</td>\n",
       "      <td>-859.516029</td>\n",
       "      <td>-872.363278</td>\n",
       "      <td>-880.776403</td>\n",
       "      <td>-858.950055</td>\n",
       "      <td>22.015116</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.092149</td>\n",
       "      <td>0.703048</td>\n",
       "      <td>1.561830</td>\n",
       "      <td>1.566615</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-819.904949</td>\n",
       "      <td>-866.805805</td>\n",
       "      <td>-869.372431</td>\n",
       "      <td>-857.945057</td>\n",
       "      <td>-853.507060</td>\n",
       "      <td>19.858023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.310222</td>\n",
       "      <td>3.984922</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-818.324497</td>\n",
       "      <td>-836.542984</td>\n",
       "      <td>-864.249680</td>\n",
       "      <td>-857.328862</td>\n",
       "      <td>-844.111506</td>\n",
       "      <td>18.045061</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.459052</td>\n",
       "      <td>6.082640</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-816.275026</td>\n",
       "      <td>-830.817184</td>\n",
       "      <td>-849.722174</td>\n",
       "      <td>-871.624163</td>\n",
       "      <td>-842.109637</td>\n",
       "      <td>20.760567</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140.812081</td>\n",
       "      <td>3.382543</td>\n",
       "      <td>0.092250</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-783.711073</td>\n",
       "      <td>-857.406269</td>\n",
       "      <td>-860.130546</td>\n",
       "      <td>-855.781006</td>\n",
       "      <td>-839.257223</td>\n",
       "      <td>32.107218</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.991968</td>\n",
       "      <td>1.283666</td>\n",
       "      <td>0.066002</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-799.476179</td>\n",
       "      <td>-839.691256</td>\n",
       "      <td>-853.287134</td>\n",
       "      <td>-841.293700</td>\n",
       "      <td>-833.437067</td>\n",
       "      <td>20.299068</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215.984421</td>\n",
       "      <td>2.642083</td>\n",
       "      <td>0.078253</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-730.235073</td>\n",
       "      <td>-861.003520</td>\n",
       "      <td>-833.880696</td>\n",
       "      <td>-832.865309</td>\n",
       "      <td>-814.496150</td>\n",
       "      <td>49.940092</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81.024556</td>\n",
       "      <td>22.375492</td>\n",
       "      <td>0.086503</td>\n",
       "      <td>0.039829</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-757.966737</td>\n",
       "      <td>-805.280708</td>\n",
       "      <td>-816.389511</td>\n",
       "      <td>-804.933369</td>\n",
       "      <td>-796.142581</td>\n",
       "      <td>22.517307</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.912800</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.085173</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-751.583551</td>\n",
       "      <td>-804.913362</td>\n",
       "      <td>-805.236779</td>\n",
       "      <td>-813.443851</td>\n",
       "      <td>-793.794386</td>\n",
       "      <td>24.609023</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97.990580</td>\n",
       "      <td>1.485266</td>\n",
       "      <td>0.074750</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-734.770261</td>\n",
       "      <td>-816.970395</td>\n",
       "      <td>-816.285486</td>\n",
       "      <td>-804.483983</td>\n",
       "      <td>-793.127531</td>\n",
       "      <td>34.056252</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107.218642</td>\n",
       "      <td>6.434560</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-746.171747</td>\n",
       "      <td>-784.257815</td>\n",
       "      <td>-811.311106</td>\n",
       "      <td>-778.099918</td>\n",
       "      <td>-779.960147</td>\n",
       "      <td>23.165001</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.414182</td>\n",
       "      <td>2.774547</td>\n",
       "      <td>0.070379</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-734.918585</td>\n",
       "      <td>-774.892424</td>\n",
       "      <td>-771.022868</td>\n",
       "      <td>-780.773757</td>\n",
       "      <td>-765.401909</td>\n",
       "      <td>17.938728</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.915283</td>\n",
       "      <td>6.262307</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-742.984840</td>\n",
       "      <td>-778.268668</td>\n",
       "      <td>-773.638945</td>\n",
       "      <td>-761.212157</td>\n",
       "      <td>-764.026152</td>\n",
       "      <td>13.655681</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91.126490</td>\n",
       "      <td>1.252946</td>\n",
       "      <td>0.514353</td>\n",
       "      <td>0.752947</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-730.274440</td>\n",
       "      <td>-778.224123</td>\n",
       "      <td>-768.719009</td>\n",
       "      <td>-773.417232</td>\n",
       "      <td>-762.658701</td>\n",
       "      <td>18.996684</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.014473</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>0.013424</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-732.919997</td>\n",
       "      <td>-767.787223</td>\n",
       "      <td>-789.349699</td>\n",
       "      <td>-757.842222</td>\n",
       "      <td>-761.974785</td>\n",
       "      <td>20.275781</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153.151929</td>\n",
       "      <td>2.407120</td>\n",
       "      <td>0.095937</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-713.970589</td>\n",
       "      <td>-769.567490</td>\n",
       "      <td>-788.304253</td>\n",
       "      <td>-766.977714</td>\n",
       "      <td>-759.705011</td>\n",
       "      <td>27.657344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>137.203365</td>\n",
       "      <td>7.334162</td>\n",
       "      <td>0.107323</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.7, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-712.122047</td>\n",
       "      <td>-786.135320</td>\n",
       "      <td>-741.342608</td>\n",
       "      <td>-757.957905</td>\n",
       "      <td>-749.389470</td>\n",
       "      <td>26.820181</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>131.874029</td>\n",
       "      <td>8.437426</td>\n",
       "      <td>0.074751</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 2, 'reg...</td>\n",
       "      <td>-716.538696</td>\n",
       "      <td>-762.183897</td>\n",
       "      <td>-759.070980</td>\n",
       "      <td>-754.320722</td>\n",
       "      <td>-748.028574</td>\n",
       "      <td>18.395050</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95.661727</td>\n",
       "      <td>1.274809</td>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'scale_pos_weight': 4, 'reg...</td>\n",
       "      <td>-708.849936</td>\n",
       "      <td>-755.094590</td>\n",
       "      <td>-746.628315</td>\n",
       "      <td>-746.929770</td>\n",
       "      <td>-739.375653</td>\n",
       "      <td>17.948330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>173.456705</td>\n",
       "      <td>8.368011</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.014601</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.9, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-707.039100</td>\n",
       "      <td>-749.671024</td>\n",
       "      <td>-746.280026</td>\n",
       "      <td>-751.990117</td>\n",
       "      <td>-738.745067</td>\n",
       "      <td>18.417735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>172.096907</td>\n",
       "      <td>2.719855</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 5, 'reg...</td>\n",
       "      <td>-705.824853</td>\n",
       "      <td>-751.704428</td>\n",
       "      <td>-750.970781</td>\n",
       "      <td>-744.214524</td>\n",
       "      <td>-738.178647</td>\n",
       "      <td>18.906251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      48.919646      0.360364         0.062261        0.004450   \n",
       "2     105.090849      4.433393         0.095831        0.032114   \n",
       "7      88.398466      4.454418         0.090253        0.013609   \n",
       "3      87.349120      6.859317         0.087999        0.021989   \n",
       "5      24.537812      0.860218         0.064751        0.018940   \n",
       "2      28.369639      0.408713         0.073752        0.011713   \n",
       "8      67.023075      0.166826         0.089501        0.019653   \n",
       "6      34.983130      0.884607         0.066703        0.022509   \n",
       "9      34.653162      5.391651         0.059999        0.015083   \n",
       "4     156.276887      1.930412         0.088130        0.014824   \n",
       "5      76.687946      8.045095         0.081751        0.011755   \n",
       "7      43.903574      1.556440         0.059249        0.010426   \n",
       "0      92.756507      0.786464         0.065499        0.006537   \n",
       "9      90.364851      6.897866         0.055500        0.010805   \n",
       "0      62.036589      0.550902         0.090500        0.014566   \n",
       "9      72.822142     12.738063         0.069749        0.022765   \n",
       "1      46.551664      4.674692         0.086752        0.020264   \n",
       "5     124.464072      8.989758         0.093300        0.027433   \n",
       "6      81.068176      8.011820         0.066516        0.008543   \n",
       "8      70.189498      7.894462         0.053494        0.009343   \n",
       "4      56.092149      0.703048         1.561830        1.566615   \n",
       "1      71.310222      3.984922         0.078000        0.009356   \n",
       "4      59.459052      6.082640         0.081001        0.008687   \n",
       "2     140.812081      3.382543         0.092250        0.009120   \n",
       "4      84.991968      1.283666         0.066002        0.011597   \n",
       "2     215.984421      2.642083         0.078253        0.028761   \n",
       "9      81.024556     22.375492         0.086503        0.039829   \n",
       "0      82.912800      0.690359         0.085173        0.005398   \n",
       "8      97.990580      1.485266         0.074750        0.012872   \n",
       "3     107.218642      6.434560         0.060500        0.007532   \n",
       "3      83.414182      2.774547         0.070379        0.005891   \n",
       "1      79.915283      6.262307         0.080500        0.014291   \n",
       "5      91.126490      1.252946         0.514353        0.752947   \n",
       "1     104.014473      0.710357         0.072253        0.013424   \n",
       "7     153.151929      2.407120         0.095937        0.011148   \n",
       "7     137.203365      7.334162         0.107323        0.022137   \n",
       "8     131.874029      8.437426         0.074751        0.018186   \n",
       "3      95.661727      1.274809         0.082250        0.005118   \n",
       "6     173.456705      8.368011         0.085561        0.014601   \n",
       "6     172.096907      2.719855         0.070335        0.013558   \n",
       "\n",
       "   param_subsample  param_scale_pos_weight  param_reg_lambda  \\\n",
       "0              0.6                       1               0.0   \n",
       "2              0.8                       3              10.0   \n",
       "7              0.6                       3               1.0   \n",
       "3              0.8                       1               0.0   \n",
       "5              0.8                       3               1.0   \n",
       "2              0.8                       4              10.0   \n",
       "8              1.0                       3              10.0   \n",
       "6              0.8                       5              10.0   \n",
       "9              0.6                       5               1.0   \n",
       "4              0.8                       5               1.0   \n",
       "5              1.0                       2               0.0   \n",
       "7              0.6                       2               1.0   \n",
       "0              1.0                       4               1.0   \n",
       "9              0.6                       1              10.0   \n",
       "0              0.7                       2               0.0   \n",
       "9              1.0                       4               1.0   \n",
       "1              1.0                       3              10.0   \n",
       "5              1.0                       4              10.0   \n",
       "6              1.0                       2               0.0   \n",
       "8              1.0                       5              10.0   \n",
       "4              0.8                       2               0.0   \n",
       "1              1.0                       4               1.0   \n",
       "4              0.6                       5               1.0   \n",
       "2              1.0                       2               1.0   \n",
       "4              0.8                       2              10.0   \n",
       "2              1.0                       5               0.0   \n",
       "9              0.7                       4              10.0   \n",
       "0              0.7                       4               1.0   \n",
       "8              1.0                       5               0.0   \n",
       "3              1.0                       2              10.0   \n",
       "3              0.7                       5               0.0   \n",
       "1              0.6                       2               0.0   \n",
       "5              0.7                       5               1.0   \n",
       "1              0.7                       5              10.0   \n",
       "7              0.7                       2              10.0   \n",
       "7              0.7                       4               0.0   \n",
       "8              0.8                       2              10.0   \n",
       "3              0.8                       4               1.0   \n",
       "6              0.9                       5               1.0   \n",
       "6              1.0                       5              10.0   \n",
       "\n",
       "   param_min_child_weight  param_max_depth  param_learning_rate  param_gamma  \\\n",
       "0                       1                5                 0.01         1.50   \n",
       "2                       5                7                 0.01         0.50   \n",
       "7                      10                7                 0.01         1.50   \n",
       "3                       5                7                 0.01         0.25   \n",
       "5                      10                2                 0.05         5.00   \n",
       "2                      10                3                 0.10         0.50   \n",
       "8                       1                5                 0.05         2.00   \n",
       "6                       1                3                 0.30         0.00   \n",
       "9                      10                4                 0.50         1.00   \n",
       "4                       1                8                 0.50         1.00   \n",
       "5                       5                5                 0.05         1.50   \n",
       "7                       1                4                 0.50         5.00   \n",
       "0                      10                6                 0.50         0.50   \n",
       "9                      10                7                 0.05         5.00   \n",
       "0                       5                6                 0.30         5.00   \n",
       "9                       5                7                 0.05         5.00   \n",
       "1                       5                4                 0.30         0.00   \n",
       "5                      10                8                 0.05         1.50   \n",
       "6                      10                5                 0.30         2.00   \n",
       "8                       5                5                 0.10         1.50   \n",
       "4                       5                5                 0.10         5.00   \n",
       "1                      10                6                 0.30         1.00   \n",
       "4                       1                5                 0.10         1.50   \n",
       "2                       5                8                 0.05         1.50   \n",
       "4                       8                6                 0.08         2.00   \n",
       "2                       5                9                 0.10         1.00   \n",
       "9                      10                7                 0.08         1.50   \n",
       "0                       8                6                 0.08         0.50   \n",
       "8                       5                7                 0.10         2.00   \n",
       "3                       5                7                 0.10         0.50   \n",
       "3                      10                6                 0.10         0.50   \n",
       "1                       5                6                 0.10         5.00   \n",
       "5                      10                6                 0.12         5.00   \n",
       "1                      10                7                 0.12         0.50   \n",
       "7                       5                9                 0.10         1.50   \n",
       "7                       8                9                 0.12         0.50   \n",
       "8                       8                9                 0.12         5.00   \n",
       "3                      10                7                 0.10         1.50   \n",
       "6                      10                9                 0.12         1.50   \n",
       "6                      10                9                 0.10         1.50   \n",
       "\n",
       "   param_colsample_bytree                                             params  \\\n",
       "0                     0.6  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "2                     1.0  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "7                     0.6  {'subsample': 0.6, 'scale_pos_weight': 3, 'reg...   \n",
       "3                     0.6  {'subsample': 0.8, 'scale_pos_weight': 1, 'reg...   \n",
       "5                     0.8  {'subsample': 0.8, 'scale_pos_weight': 3, 'reg...   \n",
       "2                     0.6  {'subsample': 0.8, 'scale_pos_weight': 4, 'reg...   \n",
       "8                     0.8  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "6                     0.8  {'subsample': 0.8, 'scale_pos_weight': 5, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "4                     1.0  {'subsample': 0.8, 'scale_pos_weight': 5, 'reg...   \n",
       "5                     0.8  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "7                     0.8  {'subsample': 0.6, 'scale_pos_weight': 2, 'reg...   \n",
       "0                     0.8  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "9                     1.0  {'subsample': 0.6, 'scale_pos_weight': 1, 'reg...   \n",
       "0                     0.6  {'subsample': 0.7, 'scale_pos_weight': 2, 'reg...   \n",
       "9                     0.6  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "1                     0.6  {'subsample': 1.0, 'scale_pos_weight': 3, 'reg...   \n",
       "5                     0.8  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "6                     1.0  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "8                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "4                     0.6  {'subsample': 0.8, 'scale_pos_weight': 2, 'reg...   \n",
       "1                     0.6  {'subsample': 1.0, 'scale_pos_weight': 4, 'reg...   \n",
       "4                     0.8  {'subsample': 0.6, 'scale_pos_weight': 5, 'reg...   \n",
       "2                     0.8  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "4                     0.8  {'subsample': 0.8, 'scale_pos_weight': 2, 'reg...   \n",
       "2                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "9                     1.0  {'subsample': 0.7, 'scale_pos_weight': 4, 'reg...   \n",
       "0                     0.8  {'subsample': 0.7, 'scale_pos_weight': 4, 'reg...   \n",
       "8                     0.6  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "3                     0.8  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg...   \n",
       "3                     1.0  {'subsample': 0.7, 'scale_pos_weight': 5, 'reg...   \n",
       "1                     0.8  {'subsample': 0.6, 'scale_pos_weight': 2, 'reg...   \n",
       "5                     1.0  {'subsample': 0.7, 'scale_pos_weight': 5, 'reg...   \n",
       "1                     1.0  {'subsample': 0.7, 'scale_pos_weight': 5, 'reg...   \n",
       "7                     0.8  {'subsample': 0.7, 'scale_pos_weight': 2, 'reg...   \n",
       "7                     0.8  {'subsample': 0.7, 'scale_pos_weight': 4, 'reg...   \n",
       "8                     0.8  {'subsample': 0.8, 'scale_pos_weight': 2, 'reg...   \n",
       "3                     0.8  {'subsample': 0.8, 'scale_pos_weight': 4, 'reg...   \n",
       "6                     1.0  {'subsample': 0.9, 'scale_pos_weight': 5, 'reg...   \n",
       "6                     1.0  {'subsample': 1.0, 'scale_pos_weight': 5, 'reg...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0       -4051.253910       -4105.651145       -4072.919965       -4115.946656   \n",
       "2       -3938.745855       -4020.001202       -3977.971223       -4033.261421   \n",
       "7       -3924.902285       -3990.921698       -3952.124769       -3988.484724   \n",
       "3       -3878.364741       -3938.255726       -3903.741477       -3934.352049   \n",
       "5       -2081.355900       -2130.765947       -2140.441487       -2157.622215   \n",
       "2       -1203.488754       -1251.913304       -1238.716929       -1260.772950   \n",
       "8       -1040.225263       -1086.958107       -1106.713273       -1113.458576   \n",
       "6       -1049.279684       -1089.646551       -1047.812713       -1078.236038   \n",
       "9       -1018.952862       -1089.429055       -1013.041958       -1077.495338   \n",
       "4       -1016.199290       -1065.083817       -1030.744470       -1070.140746   \n",
       "5        -999.903012       -1035.112921       -1054.307465       -1055.026615   \n",
       "7       -1041.048260       -1013.927135       -1038.385400       -1010.158875   \n",
       "0        -952.215285        -926.965935        -953.281038        -950.545739   \n",
       "9        -876.498722        -937.453677        -946.783762        -954.806455   \n",
       "0        -885.398644        -930.132292        -920.900980        -920.817962   \n",
       "9        -861.133416        -916.009535        -927.876145        -928.927074   \n",
       "1        -896.657547        -892.172079        -920.442263        -909.775774   \n",
       "5        -843.257513        -896.572650        -907.570839        -912.877997   \n",
       "6        -848.854772        -849.573507        -848.826881        -888.621607   \n",
       "8        -823.144510        -859.516029        -872.363278        -880.776403   \n",
       "4        -819.904949        -866.805805        -869.372431        -857.945057   \n",
       "1        -818.324497        -836.542984        -864.249680        -857.328862   \n",
       "4        -816.275026        -830.817184        -849.722174        -871.624163   \n",
       "2        -783.711073        -857.406269        -860.130546        -855.781006   \n",
       "4        -799.476179        -839.691256        -853.287134        -841.293700   \n",
       "2        -730.235073        -861.003520        -833.880696        -832.865309   \n",
       "9        -757.966737        -805.280708        -816.389511        -804.933369   \n",
       "0        -751.583551        -804.913362        -805.236779        -813.443851   \n",
       "8        -734.770261        -816.970395        -816.285486        -804.483983   \n",
       "3        -746.171747        -784.257815        -811.311106        -778.099918   \n",
       "3        -734.918585        -774.892424        -771.022868        -780.773757   \n",
       "1        -742.984840        -778.268668        -773.638945        -761.212157   \n",
       "5        -730.274440        -778.224123        -768.719009        -773.417232   \n",
       "1        -732.919997        -767.787223        -789.349699        -757.842222   \n",
       "7        -713.970589        -769.567490        -788.304253        -766.977714   \n",
       "7        -712.122047        -786.135320        -741.342608        -757.957905   \n",
       "8        -716.538696        -762.183897        -759.070980        -754.320722   \n",
       "3        -708.849936        -755.094590        -746.628315        -746.929770   \n",
       "6        -707.039100        -749.671024        -746.280026        -751.990117   \n",
       "6        -705.824853        -751.704428        -750.970781        -744.214524   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0     -4086.442919       25.790323               10  \n",
       "2     -3992.494925       37.142977                9  \n",
       "7     -3964.108369       27.358202                8  \n",
       "3     -3913.678498       24.378483                7  \n",
       "5     -2127.546387       28.349452                6  \n",
       "2     -1238.722984       21.803879               10  \n",
       "8     -1086.838805       28.620112                5  \n",
       "6     -1066.243747       18.158942                9  \n",
       "9     -1049.729803       34.059410                8  \n",
       "4     -1045.542081       22.731819                7  \n",
       "5     -1036.087503       22.365845               10  \n",
       "7     -1025.879917       13.932748                6  \n",
       "0      -945.751999       10.889865                5  \n",
       "9      -928.885654       30.862728                4  \n",
       "0      -914.312470       17.117287                9  \n",
       "9      -908.486543       27.805962                8  \n",
       "1      -904.761916       11.126509                3  \n",
       "5      -890.069750       27.659435                4  \n",
       "6      -858.969192       17.122446                7  \n",
       "8      -858.950055       22.015116                6  \n",
       "4      -853.507060       19.858023                5  \n",
       "1      -844.111506       18.045061                4  \n",
       "4      -842.109637       20.760567                2  \n",
       "2      -839.257223       32.107218                3  \n",
       "4      -833.437067       20.299068               10  \n",
       "2      -814.496150       49.940092                9  \n",
       "9      -796.142581       22.517307                8  \n",
       "0      -793.794386       24.609023                7  \n",
       "8      -793.127531       34.056252                3  \n",
       "3      -779.960147       23.165001                6  \n",
       "3      -765.401909       17.938728                2  \n",
       "1      -764.026152       13.655681                2  \n",
       "5      -762.658701       18.996684                5  \n",
       "1      -761.974785       20.275781                4  \n",
       "7      -759.705011       27.657344                1  \n",
       "7      -749.389470       26.820181                3  \n",
       "8      -748.028574       18.395050                2  \n",
       "3      -739.375653       17.948330                1  \n",
       "6      -738.745067       18.417735                1  \n",
       "6      -738.178647       18.906251                1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv('models/xgb-random-grid-search-results-cycle1.csv'), pd.read_csv('models/xgb-random-grid-search-results-cycle2.csv'), pd.read_csv('models/xgb-random-grid-search-results-cycle3.csv'), pd.read_csv('models/xgb-random-grid-search-results-cycle4.csv')])\n",
    "df.sort_values('mean_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea142e2-5180-4f2c-aef7-f3e1f3ff8725",
   "metadata": {},
   "source": [
    "### Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c235053c-e244-4d1e-832d-f5d31cad3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "xgb = XGBRegressor(\n",
    "    verbosity=1, \n",
    "    n_estimators=300, \n",
    "    subsample=1.0, \n",
    "    scale_pos_weight=5, \n",
    "    reg_lambda=10.0, \n",
    "    min_child_weight=10, \n",
    "    max_depth=9, \n",
    "    learning_rate=0.1, \n",
    "    gamma=1.5, \n",
    "    colsample_bytree=1.0,\n",
    "    n_jobs=5)\n",
    "xgb.fit(X_train, y_train)\n",
    "with open('models/ensemble_xg_boosting.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b597be-4ebf-4662-b5a6-67bffb5c70ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 300,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'max_depth': 9,\n",
       " 'learning_rate': 0.1,\n",
       " 'verbosity': 1,\n",
       " 'booster': None,\n",
       " 'tree_method': None,\n",
       " 'gamma': 1.5,\n",
       " 'min_child_weight': 10,\n",
       " 'max_delta_step': None,\n",
       " 'subsample': 1.0,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': 10.0,\n",
       " 'scale_pos_weight': 5,\n",
       " 'base_score': None,\n",
       " 'missing': nan,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'n_jobs': 5,\n",
       " 'monotone_constraints': None,\n",
       " 'interaction_constraints': None,\n",
       " 'importance_type': None,\n",
       " 'gpu_id': None,\n",
       " 'validate_parameters': None,\n",
       " 'predictor': None,\n",
       " 'enable_categorical': False,\n",
       " '_Booster': <xgboost.core.Booster at 0x225638c1340>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c468d8a-38b3-4342-a2a8-863a92272d12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05c60d13-ef90-4c90-9512-881af47abe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance\n",
    "\n",
    "from xgboost import plot_importance\n",
    "# https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.plotting\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12,6))\n",
    "\n",
    "plot_importance(\n",
    "    xgb, \n",
    "    ax=axs[0], \n",
    "    height=0.2, \n",
    "    xlim=None,#(0, 600), \n",
    "    ylim=None, \n",
    "    title='Feature importance (weight)', \n",
    "    xlabel='F score', \n",
    "    ylabel='Features', \n",
    "    fmap='', \n",
    "    importance_type='weight', \n",
    "    max_num_features=10, \n",
    "    grid=False, \n",
    "    show_values=False, \n",
    "    #**kwargs\n",
    ")\n",
    "\n",
    "plot_importance(\n",
    "    xgb, \n",
    "    ax=axs[1], \n",
    "    height=0.2, \n",
    "    xlim=None,#(0, 600), \n",
    "    ylim=None, \n",
    "    title='Feature importance (gain)', \n",
    "    xlabel='F score', \n",
    "    #ylabel='Features', \n",
    "    fmap='', \n",
    "    importance_type='gain', \n",
    "    max_num_features=10, \n",
    "    grid=False, \n",
    "    show_values=False, \n",
    "    #**kwargs\n",
    ")\n",
    "\n",
    "plot_importance(\n",
    "    xgb, \n",
    "    ax=axs[2], \n",
    "    height=0.2, \n",
    "    xlim=None,#(0, 600), \n",
    "    ylim=None, \n",
    "    title='Feature importance (cover)', \n",
    "    xlabel='F score', \n",
    "    #ylabel='Features', \n",
    "    fmap='', \n",
    "    importance_type='cover', \n",
    "    max_num_features=10, \n",
    "    grid=False, \n",
    "    show_values=False, \n",
    "    #**kwargs\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05796d-c82d-4ad3-90ba-3fa3b37f61a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
